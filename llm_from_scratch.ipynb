{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2757bea-ce81-4bac-a106-6765790401d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter 2 - text prerpocessing, tokenization, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cdd03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x10468f950>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "\"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d165cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf28145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc41c52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152ae936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f581d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n",
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "print(result)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e286bead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying it to the dataset\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ba15217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'HAD',\n",
       " 'always',\n",
       " 'thought',\n",
       " 'Jack',\n",
       " 'Gisburn',\n",
       " 'rather',\n",
       " 'a',\n",
       " 'cheap',\n",
       " 'genius']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ca15d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "933fffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2674fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text) # to numerical values \n",
    "        preprocessed = [\n",
    "        item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids): # back to natural language\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd219693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8c785a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "368ed6f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      6\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text) \u001b[38;5;66;03m# to numerical values \u001b[39;00m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      9\u001b[0m ]\n\u001b[0;32m---> 10\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text)) # error due to the text is not in training set. The word 'hello' not used in the short story 'The verdict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1616306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding special context tokens :- that is we add <|unk|> to new or unknown words while <|endoftext|> for sentence completionb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ec28495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "# modifying the vocabulary \n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ef4f2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21849a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "        item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "        else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4bae2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90b424fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text)) # 1131 = <|unk|> and 1130 = <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13a9b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65adf89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66d5d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tik_tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99e09bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "\"of someunknownPlace.\"\n",
    ")\n",
    "integers = tik_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0835db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tik_tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6afbf9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959, 13]\n"
     ]
    }
   ],
   "source": [
    "unk = \"Akwirw ier.\"\n",
    "print(tik_tokenizer.encode(unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac9c48a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akwirw ier.\n"
     ]
    }
   ],
   "source": [
    "print(tik_tokenizer.decode(tik_tokenizer.encode(unk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "095d7f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "# now tokenize the short story dataset using the BPE tokenizer\n",
    "enc_text = tik_tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f800643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5095"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "len(enc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fef225f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y: [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "263a9cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6abfba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tik_tokenizer.decode(context), \"---->\", tik_tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56e94067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tik_tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4384763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "stride=128, shuffle=True, drop_last=True,\n",
    "num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            drop_last=drop_last,\n",
    "                            num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ea39b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c47f5680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "second_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb95ceda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4,\n",
    "    shuffle=False\n",
    "    )\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22b9c570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# token IDs into token embedding vectors\n",
    "\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bef3596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3]))) # embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40931bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b16f42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usefull embdng sizes and encode the input tokens into a\n",
    "# 256-dimensional vector representation, which is smaller than what the original GPT-3\n",
    "# model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable\n",
    "# for experimentation. furthermore, we assume that the token IDs were created by the\n",
    "# BPE tokenizer we implemented earlier, which has a vocabulary size of 50,257:\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "655189de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "raw_text, batch_size=8, max_length=max_length,\n",
    "stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ff4e355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f503675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd6fdb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d9c3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter 3 -  self-attention intro and MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "076db777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # your     (x^1)\n",
    " [0.55, 0.87, 0.66], # journey  (x^2)\n",
    " [0.55, 0.87, 0.66], # starts   (x^3)\n",
    " [0.22, 0.58, 0.33], # with     (x^4)\n",
    " [0.77, 0.25, 0.10], # one      (x^5)\n",
    " [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6a7adb5-77ff-415f-9861-b679ca1a80a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8eb9d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] \n",
    "d_in = inputs.shape[1] # input embedding size , d = 3\n",
    "d_out = 2 # output embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2b1f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6043e426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2961, 0.5166],\n",
       "        [0.2517, 0.6886],\n",
       "        [0.0740, 0.8665]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ca54d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "157fb950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c973cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3669, 0.7646],\n",
       "        [0.4433, 1.1419],\n",
       "        [0.4433, 1.1419],\n",
       "        [0.2408, 0.6706],\n",
       "        [0.1827, 0.3292],\n",
       "        [0.3275, 0.9642]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f4b96fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22) # unnormalized attention score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06191791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8524, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # computaton to all attention scores\n",
    "print(attn_scores_2) # 2nd element matches we computed prev (attn_scores_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e1e71ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1490, 0.2249, 0.2249, 0.1302, 0.0900, 0.1808])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1] \n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1) # qk^T / root_dk(dim of the key matrix)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "203e02dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3082, 0.8267])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2697e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compact self-attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06eee1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax( attn_scores / keys.shape[-1]**0.5, dim=-1 )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7b6b9754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3015, 0.8104],\n",
      "        [0.3082, 0.8267],\n",
      "        [0.3082, 0.8267],\n",
      "        [0.2965, 0.7986],\n",
      "        [0.2944, 0.7936],\n",
      "        [0.3009, 0.8091]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs)) # inputs contains 6 embedding vectors , results in a matrix storing 6 context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f250d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention using linear layer.\n",
    "# instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear\n",
    "# has an optimized weight initialization scheme, contributing to more stable and\n",
    "# effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1eadaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax( attn_scores / keys.shape[-1]**0.5, dim=-1 )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8bd18043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5340, -0.1049],\n",
      "        [-0.5326, -0.1078],\n",
      "        [-0.5326, -0.1078],\n",
      "        [-0.5300, -0.1074],\n",
      "        [-0.5313, -0.1064],\n",
      "        [-0.5302, -0.1079]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs)) # SelfAttention_v1 and SelfAttention_v2 give different outputs because\n",
    "                     # they use different initial weights for the weight matrices since nn.Linear uses a more\n",
    "                     # sophisticated weight initialization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e5fd9de-a9c2-407b-9372-5b82f24d6398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1716, 0.1762, 0.1762, 0.1555, 0.1626, 0.1579],\n",
      "        [0.1635, 0.1749, 0.1749, 0.1611, 0.1604, 0.1651],\n",
      "        [0.1635, 0.1749, 0.1749, 0.1611, 0.1604, 0.1651],\n",
      "        [0.1636, 0.1703, 0.1703, 0.1651, 0.1632, 0.1674],\n",
      "        [0.1667, 0.1722, 0.1722, 0.1617, 0.1633, 0.1639],\n",
      "        [0.1624, 0.1708, 0.1708, 0.1654, 0.1624, 0.1681]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "                                # Reuses the query and key weight matrices\n",
    "                                # of the SelfAttention_v2 object from the\n",
    "                                # previous section for convenience\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c7ed6723-9995-4b00-8bf6-1fdb6b93a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length)) #tril fn to create a mask where the values above the diagonal are zero\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a07c154e-7a48-44fc-b872-d57ad78c0494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1716, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1635, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1635, 0.1749, 0.1749, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1636, 0.1703, 0.1703, 0.1651, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1722, 0.1722, 0.1617, 0.1633, 0.0000],\n",
       "        [0.1624, 0.1708, 0.1708, 0.1654, 0.1624, 0.1681]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f89644d0-f015-4b18-b5a8-8ac58f24bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3186, 0.3407, 0.3407, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2444, 0.2544, 0.2544, 0.2467, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2060, 0.1934, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1708, 0.1708, 0.1654, 0.1624, 0.1681]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm) # renormalize the attention weights to sum up to 1 again in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "217f7b26-d4b4-46aa-824b-55675724fd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1716],\n",
       "        [0.3384],\n",
       "        [0.5133],\n",
       "        [0.6694],\n",
       "        [0.8361],\n",
       "        [1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d123de43-a8f5-444e-9847-281df10b8535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.2602, 0.2602,   -inf,   -inf,   -inf],\n",
      "        [0.0510, 0.1080, 0.1080, 0.0643,   -inf,   -inf],\n",
      "        [0.1415, 0.1875, 0.1875, 0.0987, 0.1121,   -inf],\n",
      "        [0.0476, 0.1192, 0.1192, 0.0731, 0.0477, 0.0966]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91e397e7-4fa7-40ca-8530-2d14edfd57fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3186, 0.3407, 0.3407, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2444, 0.2544, 0.2544, 0.2467, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2060, 0.1934, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1708, 0.1708, 0.1654, 0.1624, 0.1681]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "43afff34-938f-4468-93f7-4efbee9c25ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a2dc2a6-0cc5-4f13-89d1-5667e20193a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6373, 0.6814, 0.6814, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5089, 0.5089, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4119, 0.0000, 0.3869, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3417, 0.3417, 0.3307, 0.3248, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e4d7304f-762a-48a3-af6f-9b4af931589e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0466e467-dbd7-4d70-8f80-33025cd5dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer( # buffers are used automatically move to the apropriate device (cpu or gpu) along with the model\n",
    "        'mask',\n",
    "        torch.triu(torch.ones(context_length, context_length),\n",
    "        diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c6f334bc-5c8e-4028-9bdd-d1ef0c5a3c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5845e9e3-810b-4c0a-b532-ee12f357a6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6306, -0.0630],\n",
       "         [-0.5679, -0.0840],\n",
       "         [-0.5529, -0.0979],\n",
       "         [-0.5302, -0.1079]],\n",
       "\n",
       "        [[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6306, -0.0630],\n",
       "         [-0.5679, -0.0840],\n",
       "         [-0.5529, -0.0979],\n",
       "         [-0.5302, -0.1079]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "675d9900-5a95-4964-bf21-8540f21532f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "        dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(\n",
    "                d_in, d_out, context_length, dropout, qkv_bias\n",
    "            )\n",
    "            for _ in range(num_heads)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1) # processing sequentially not simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fb17d73c-275e-47d8-8d50-9d2509ef6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67f2ab62-9815-4e46-8e1d-e05788b88e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6306, -0.0630,  0.6206,  0.3875],\n",
      "         [-0.5679, -0.0840,  0.5479,  0.3597],\n",
      "         [-0.5529, -0.0979,  0.5322,  0.3434],\n",
      "         [-0.5302, -0.1079,  0.5077,  0.3499]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6306, -0.0630,  0.6206,  0.3875],\n",
      "         [-0.5679, -0.0840,  0.5479,  0.3597],\n",
      "         [-0.5529, -0.0979,  0.5322,  0.3434],\n",
      "         [-0.5302, -0.1079,  0.5077,  0.3499]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "da2dd240-ead2-4617-b3c5-f7ca33ec8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor is two because we have 2 input texts  (the input texts are duplicated, which is why the context vectors are exactly the same for those)\n",
    "# second dim refers to the 6 tokens in each input\n",
    "# third dim refers to the 4 dim embedding of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "914f4428-55e0-48d1-b7d8-59396aff2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "        context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # reduces the projection dim to match the desired output dim\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # uses a Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                diagonal=1)\n",
    "        )\n",
    "        \n",
    "    # tensor shape: (b, num_tokens, d_out)\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # we implicitly split the matrix by adding a num_heads dimension.\n",
    "        #then we unroll the last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim).\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(\n",
    "            b, num_tokens, self.num_heads, self.head_dim\n",
    "        )\n",
    "        #transposes from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2) \n",
    "        values = values.transpose(1, 2)\n",
    "    \n",
    "        attn_scores = queries @ keys.transpose(2, 3) # computes dot for each head\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # masks truncated to the no of tokens\n",
    "    \n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) # uses mask to fill attn_scores\n",
    "    \n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "    \n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # tensor shape: (b, num_tokens, n_heads, head_dim)\n",
    "    \n",
    "        # Combines heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "        b, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec) # adds an optional linear projection\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "23dd97cd-c7d2-43f4-a3d1-cc059b04fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "[0.8993, 0.0390, 0.9268, 0.7388],\n",
    "[0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "[[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "[0.4066, 0.2318, 0.4545, 0.9737],\n",
    "[0.4606, 0.5159, 0.4220, 0.5786]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f7674f3a-06a9-455d-9247-6c0d74490949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "print(a @ a.transpose(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "68f97b6e-b2d4-4c4b-9e7c-c9adbc1e963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aaaf6fde-5c54-4218-bd67-b9af137c8422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "47b51e7d-0abd-49cf-b23a-e1c579fb173e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2857, 0.3590],\n",
      "         [0.2694, 0.3872],\n",
      "         [0.2640, 0.3926],\n",
      "         [0.2576, 0.4027]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2857, 0.3590],\n",
      "         [0.2694, 0.3872],\n",
      "         [0.2640, 0.3926],\n",
      "         [0.2576, 0.4027]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ec7954f1-0ccd-45c0-9c0c-db68e1ea620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter 4 - LLM architecture :-  implementing gpt-2 , transformers 124m (also diff block as exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8e9d53e8-2861-47eb-bc5e-042e07d45c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257,     # Vocabulary size\n",
    "\"context_length\": 1024,  # Context length\n",
    "\"emb_dim\": 768,          # Embedding dimension\n",
    "\"n_heads\": 12,           # Number of attention heads\n",
    "\"n_layers\": 12,          # Number of layers\n",
    "\"drop_rate\": 0.1,        # Dropout rate\n",
    "\"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3e7ba913-663b-4c75-9360-2adea3b5eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg)\n",
    "            for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "        torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c4faf8a-c314-4513-8b83-a40fad379fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a39d5c85-5114-44f4-bfe2-e7721089b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e7759b6-a2df-49da-acbc-656dbec2d582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
       "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
       "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
       "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
       "\n",
       "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
       "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
       "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
       "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch) # logits are unormalized model's pred before an actvn fn is applied , these values can range from -ve to +ve infinity\n",
    "print(\"Output shape:\", logits.shape) # and represent the model's confidence in assigning an input to a particular class.\n",
    "logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b0984587-c3a0-4265-974d-a1542896e6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
       "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c39e5ef9-4493-4f33-b993-9bde7e6199eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
       "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a446f493-924a-4562-8ccb-7a325e14539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e377b37f-8ccf-4f12-9823-1c98855a3b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6e78413d-5d53-4502-8a1b-b975de32c174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False) \n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "29cbe8ec-7400-45b3-8f1f-39f80262737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9f45353e-31dd-414b-82be-2ee2683c30b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "44d8c09a-fe44-40b5-8e3e-c5640f4ec872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7927c57e-edaa-4e4c-9133-3e7b0f85ce9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "# plt.tight_lay\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bf15cc53-e431-443c-b95b-41d4637c23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The smoothness of GELU can lead to better optimization properties during training,\n",
    "# as it allows for more nuanced adjustments to the models parameters. In contrast,\n",
    "# ReLU has a sharp corner at zero , which can sometimes make opti-mization harder, \n",
    "# especially in networks that are very deep or have complex architectures.\n",
    "# Moreover, unlike ReLU, which outputs zero for any negative input, GELU\n",
    "# allows for a small, non-zero output for negative values. This characteristic means that\n",
    "# during the training process, neurons that receive negative input can still contribute to\n",
    "# the learning process, albeit to a lesser extent than positive inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e0f4c6f5-92f4-4a65-9463-6f37b71d76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c7b2fc53-be3a-4f01-9fd1-7a5fd201299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b1b76820-7c62-4680-80a8-ab66002a5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut Connections or Skip Conncetions or Residual Connections\n",
    "# as the layer progress there's a high chance of problem like vanishing gradient.\n",
    "# vanshing gradient :- vanishing gradient problem refers to the issue where gradients\n",
    "#     (which guide weight updates during training) become progressively smaller as they\n",
    "#     propagate backward through the layers, making it difficult to effectively train earlier\n",
    "#     layers.\n",
    "# to prevent vanishing gradiant prblem  the soln is  skip or residual conncetions :-\n",
    "#         creates alternative or shotcut path for grdient flow  thruogh the network by skipping one or more layers,\n",
    "#         which is achieved by adding output of one layer to the output of a later layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c3dbe90-0a74-4077-82b0-d49381f11107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "        nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]),\n",
    "        GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]),\n",
    "        GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]),\n",
    "        GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]),\n",
    "        GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]),\n",
    "        GELU())\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x) # compute the output of the current layer\n",
    "            if self.use_shortcut and x.shape == layer_output.shape: # check if shortcuts can be applied\n",
    "                x = x + layer_output \n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "079a9ae0-d5e5-4d87-977d-3f36db3e1199",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specifies random seeds for initial weights for reproducibility\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d40838b7-ddfc-4a2a-a838-d029f84c3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x) # fwd pass\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target) # calculates loss based on how close the target and output are\n",
    "    loss.backward() # backward pass to calculate the gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2d7b6d71-3d98-4929-bab5-af6ceafacfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
      "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1b36521f-694d-4f53-9630-670d5b1d385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the gradient becomes smaller when we progress from the last layer (vanishing gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "75b160e8-82cd-492c-b8d2-0668467ecc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True # applying skip connections\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fc63b10e-2815-41fb-991f-366d3c0b8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        shortcut = x # shortcut conn for attention block\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # add the orginal input back\n",
    "\n",
    "        shortcut = x # shortcut conn for ff block\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # adds the orginal back\n",
    "        return x\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9bc3d7ca-9958-4f27-958a-9098c995a35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) # smaple i/p shape [batch_size, num_tokens, num_emb]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b77e0173-e03b-4726-b267-f7900e4fb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb( \n",
    "        torch.arange(seq_len, device=in_idx.device) # device setting will allow us to train the model on a CPU or GPU, depending on which device the input data sits on.\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9b97256e-7839-4fa6-85f2-38aaf920fd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4223, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c7fe94ce-5818-4514-8726-ccd3cfc12ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters()) # analyzing size using numel() - number of elements, we can collect\n",
    "print(f\"Total number of parameters: {total_params:,}\") # total number of parameters in the model's parameter tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "693db8f9-7654-4865-b260-047c036b5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why 163m instead of 124m ? - reason is a concept called \"weight tying\" which was used in the original GPT-2 architecture.\n",
    "# It means that the original GPT-2 architecture reuses the weights from the tokn embdng layer in its output layer.\n",
    "# To understand better, lets take a look at the shapes of the token emdng layer and linear ouput layer that we initialzed\n",
    "# on the model via the GPTModel earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dbe101d4-9bd7-4949-acc3-04905dbd182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8fdf2d24-06b4-4436-9f21-17f48a852366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token embedding and output layers are very large due to the number of rows for\n",
    "# the 50,257 in the tokenizers vocabulary. Lets remove the output layer parameter\n",
    "# count from the total GPT-2 model count according to the weight tying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2490e3f1-d129-455f-a747-f310f1d7b46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "total_params - sum(p.numel()\n",
    "for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {total_params_gpt2:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e612687a-808b-4e68-a730-a53b1e3e62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parameters in feed forward and attention modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0115dcda-0ea1-4b2f-9fca-dff6b16c1275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feed fwd parameteres: 4,722,432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FeedForward(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn_param = sum(p.numel() for p in ffn.parameters())\n",
    "print(f\"feed fwd parameteres: {ffn_param:,}\")\n",
    "ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e01cb27d-5028-41cb-8831-d20effab5f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttn parameters: 24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (W_query): Linear(in_features=3, out_features=2, bias=False)\n",
       "  (W_key): Linear(in_features=3, out_features=2, bias=False)\n",
       "  (W_value): Linear(in_features=3, out_features=2, bias=False)\n",
       "  (out_proj): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_param = sum(p.numel() for p in mha.parameters())\n",
    "print(f\"MultiHeadAttn parameters: {mha_param:,}\")\n",
    "mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a8982014-6b86-4af3-bb01-ca0551d7d879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4718592"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(768 * 3072) * 2 # ffnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "16109225-ae43-4b73-a3b0-202ffa6d5634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596736"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((3 * 768) * 3) + (768 * 768) # mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "650fbe76-6315-4880-80a4-2403ae4341ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# lastly compute the memory req for 163m\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "233c722a-e016-4bc9-9cbe-d61587771ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing larger gpt models, first gpt_medium with 24 tfr blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e009f7aa-08bd-4dbb-a0ac-2c9ce1cfc773",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_MEDIUM = {\n",
    "\"vocab_size\": 50257,     # Vocabulary size\n",
    "\"context_length\": 1024,  # Context length\n",
    "\"emb_dim\": 1024,          # Embedding dimension\n",
    "\"n_heads\": 16,           # Number of attention heads\n",
    "\"n_layers\": 24,          # Number of layers\n",
    "\"drop_rate\": 0.1,        # Dropout rate\n",
    "\"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8a97101f-bf3a-45aa-8b6d-296c1b82eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb( \n",
    "        torch.arange(seq_len, device=in_idx.device) # device setting will allow us to train the model on a CPU or GPU, depending on which device the input data sits on.\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "94ae36e4-01df-4e40-aca1-fbf19764d43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5553,  0.4949,  0.3092,  ..., -0.2539, -0.3032,  0.2971],\n",
       "         [ 0.3024, -0.0328, -0.5984,  ..., -0.6252, -0.5445, -0.9409],\n",
       "         [ 0.4388,  0.3456, -0.2936,  ...,  0.5144,  0.0907,  0.0242],\n",
       "         [ 0.1325,  0.6145, -0.8940,  ...,  0.3889,  0.0279, -0.5072]],\n",
       "\n",
       "        [[ 1.0220,  0.3779,  0.8361,  ...,  0.1372, -0.2403,  0.8152],\n",
       "         [ 0.0192,  0.3544, -0.5428,  ..., -0.0214,  0.0261, -0.4027],\n",
       "         [ 0.2641,  0.1448, -0.3102,  ..., -0.0764, -0.4402,  0.2604],\n",
       "         [ 0.6284,  1.2054, -0.6994,  ...,  0.0841, -0.3732, -0.1643]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "gpt_medium = GPTModel(GPT_CONFIG_MEDIUM)\n",
    "medium_out = gpt_medium(batch)\n",
    "print(medium_out.shape)\n",
    "medium_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b13839b4-1ce5-44f7-b46f-649ae432a397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406,212,608\n"
     ]
    }
   ],
   "source": [
    "ttl_gpt_medium_params = sum(p.numel() for p in gpt_medium.parameters()) # small model has 163,009,536 , 24 block has 406m\n",
    "print(f\"{ttl_gpt_medium_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4ee869de-c323-4a21-9551-e0439754036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 111,546,368\n"
     ]
    }
   ],
   "source": [
    "ttl_gpt_medium_params = (\n",
    "total_params - sum(p.numel()\n",
    "for p in gpt_medium.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {ttl_gpt_medium_params:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "81e615c9-fdef-4d0e-b7ca-7680a3184507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt_large  with 36 tfr blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "29703235-bb86-42ad-9529-990a788ff598",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_LARGE = {\n",
    "\"vocab_size\": 50257,     # Vocabulary size\n",
    "\"context_length\": 1024,  # Context length\n",
    "\"emb_dim\": 1280,          # Embedding dimension\n",
    "\"n_heads\": 20,           # Number of attention heads\n",
    "\"n_layers\": 36,          # Number of layers\n",
    "\"drop_rate\": 0.1,        # Dropout rate\n",
    "\"qkv_bias\": False        # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb( \n",
    "        torch.arange(seq_len, device=in_idx.device) # device setting will allow us to train the model on a CPU or GPU, depending on which device the input data sits on.\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "04a6f4e5-bc6e-4cf7-ac75-f2edca7c5c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[    -0.1292,     -0.3847,      0.2248,  ...,      0.5369,\n",
       "               0.3691,     -0.7619],\n",
       "         [     0.4239,      0.2338,     -1.0530,  ...,     -0.0855,\n",
       "              -0.6005,     -0.4725],\n",
       "         [     0.2074,     -0.6801,     -0.0454,  ...,     -1.2456,\n",
       "              -0.8294,     -0.0187],\n",
       "         [     0.4618,     -0.2833,     -0.4043,  ...,      0.0392,\n",
       "              -0.5204,     -0.2321]],\n",
       "\n",
       "        [[    -0.6438,      0.1008,      0.2805,  ...,      0.5632,\n",
       "               0.7455,     -0.3469],\n",
       "         [     0.5257,     -0.0004,     -0.1389,  ...,     -0.3714,\n",
       "               0.3727,      0.1004],\n",
       "         [     0.3873,     -0.7259,     -0.1061,  ...,     -1.1120,\n",
       "              -0.4745,      0.2956],\n",
       "         [     0.2372,     -0.0430,      0.1782,  ...,     -0.4974,\n",
       "              -0.2305,      0.2401]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "gpt_large = GPTModel(GPT_CONFIG_LARGE)\n",
    "large_out = gpt_large(batch)\n",
    "print(large_out.shape)\n",
    "large_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4ece34b0-2304-47fc-a119-7283ad72061b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838,220,800\n"
     ]
    }
   ],
   "source": [
    "ttl_gpt_large_params = sum(p.numel() for p in gpt_large.parameters()) \n",
    "print(f\"{ttl_gpt_large_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c6c1d232-9987-46eb-836c-c6c4980331f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 98,680,576\n"
     ]
    }
   ],
   "source": [
    "ttl_gpt_large_params = (\n",
    "total_params - sum(p.numel()\n",
    "for p in gpt_large.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {ttl_gpt_large_params:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8bb153c3-9658-4482-8b66-bde4f8ce81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt xl with tfr blocks 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "963ec260-92d9-4bbb-ae8a-5433d1f578f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_XL = {\n",
    "\"vocab_size\": 50257,     # Vocabulary size\n",
    "\"context_length\": 1024,  # Context length\n",
    "\"emb_dim\": 1600,          # Embedding dimension\n",
    "\"n_heads\": 25,           # Number of attention heads\n",
    "\"n_layers\": 48,          # Number of layers\n",
    "\"drop_rate\": 0.1,        # Dropout rate\n",
    "\"qkv_bias\": False        # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb( \n",
    "        torch.arange(seq_len, device=in_idx.device) # device setting will allow us to train the model on a CPU or GPU, depending on which device the input data sits on.\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2a596413-ceaa-48ff-821b-11d395c2382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1291,  0.3885,  0.1057,  ...,  0.0180,  0.2309, -0.3901],\n",
       "         [-0.2870, -0.0277, -0.3349,  ..., -0.0724,  0.1030, -0.3322],\n",
       "         [-0.6001, -0.2720,  0.3672,  ...,  0.0804,  0.6928,  0.0880],\n",
       "         [-1.3517, -0.3163,  0.2312,  ...,  0.3396, -0.0351,  0.3853]],\n",
       "\n",
       "        [[-0.1893,  0.3246, -0.1917,  ..., -0.2170,  0.2229, -0.6889],\n",
       "         [ 0.0024, -0.1743, -0.2006,  ...,  0.0684, -0.9490, -0.8778],\n",
       "         [-0.0397,  0.0353,  0.2041,  ..., -0.3985,  0.1638, -0.2869],\n",
       "         [-0.7979,  0.2167, -0.0873,  ...,  0.9055, -0.4025, -0.0753]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "gpt_xl = GPTModel(GPT_CONFIG_XL)\n",
    "xl_out = gpt_xl(batch)\n",
    "print(xl_out.shape)\n",
    "xl_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a5a1ecd0-0d0d-4e5c-af29-37eaf09be860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,637,792,000\n"
     ]
    }
   ],
   "source": [
    "ttl_gpt_large_params = sum(p.numel() for p in gpt_xl.parameters()) \n",
    "print(f\"{ttl_gpt_large_params:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cbdd5fd4-4686-4f69-aabf-de642a1f6eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 82,598,336\n"
     ]
    }
   ],
   "source": [
    "ttl_gpt_xl_params = (\n",
    "total_params - sum(p.numel()\n",
    "for p in gpt_xl.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {ttl_gpt_xl_params:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cee9a3e1-7298-4fe5-93b6-925ee1566acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, # dx is a (batch, n_tokens) array of indices in the current context.\n",
    "    max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:] # crops current context if exceeds the supported context size, if llm supports only 5 tokens, and the context is 10, then only the last 5 tokens are used as context\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :] # Focuses only on the last time step, so that (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        probas = torch.softmax(logits, dim=-1) # prob has shape (batch, vocab_size).\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True) # idx next has shape(batch, 1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # appends sampled text to the running sequence, where idx has shape(batch, n_tokens+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "26c4d0cf-757f-4bb7-894c-bcced50fa51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) # adds batch dim\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f746bf90-184f-495f-a44e-35ef4c5fc319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Disables dropout since we are not training the model\n",
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f7cb41fd-1ac1-4e7e-8c3a-6cd7d7a0cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5c3a9514-ef1d-4bb4-86b9-08cb6901163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter 5 - pretraining , model evaluation, taking pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5aca1d3f-e951-451a-9ab7-76279608ccb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# from chapter04 import GPTModel\n",
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257,\n",
    "\"context_length\": 256, # 1024 to 256\n",
    "\"emb_dim\": 768,\n",
    "\"n_heads\": 12,\n",
    "\"n_layers\": 12,\n",
    "\"drop_rate\": 0.1, # Its possible and common to set dropout to 0.\n",
    "\"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c3376f5b-cbb0-4aff-a591-adf252d7c510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasn refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "# from chapter04 import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # .unsqueeze(0) adds the batch dimension\n",
    "    return encoded_tensor\n",
    "    \n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # removes batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "model=model,\n",
    "idx=text_to_token_ids(start_context, tokenizer),\n",
    "max_new_tokens=10,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8950ef74-c29a-4fca-854a-60414e2e28b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100], # [\"every effort moves\",\n",
    "[40, 1107, 588]]) # \"I really like\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d3bcd6f1-ca08-46a2-b298-a63e215f7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([[3626, 6100, 345 ], # [\" effort moves you\",\n",
    "[1107, 588, 11311]]) # \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "45f5c3ca-944f-4a54-bf30-bf5fcb2d0063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # not training so dsable gradient tracking\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1) # prob of each token in vocabulary\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "136089e3-ccb0-4753-9b17-7d81c45de1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7de8f2ab-f061-40b3-8d38-f5adfd567ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:   Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: \"f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "769ef434-7606-4155-9bd4-73c2cc768a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0001,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0001,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "397e098a-489b-4529-9c17-24cf1f9ce536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "188c67c0-0211-4719-b789-769477cbca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "27cee3a6-88ba-4bfa-8fe8-6b850d2bd53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cd71ae2a-fad0-4cfa-aaa9-00332cdb7918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9bd9964e-65ed-4ef0-9aea-42e2768798df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "284cd5f2-f015-4bde-a334-c70ab4f69668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2697dea0-1705-4153-ae46-43354291e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fdd0b4da-74d3-4227-a261-257293148c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "70bfa3c9-8952-4a12-9a44-215164cb2965",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a29cadd2-9e2c-42ac-9caf-a231778a0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chapter02 import create_dataloader_v1\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "77490307-a8e1-4687-931e-b99aa97004f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "08313a03-eea9-4e5b-8fd6-b194041583b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device) # the transfer to a given device allows us to transfer the data to a GPU.\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "    logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d0d990ca-2ad4-46a1-a3f7-558924207bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader) # iteratives over all batches if no fixed num_batches is specified\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader)) # reduces the number of batches to match the total number of batches in the data loader if num_batches exceeds the number of batches in the data loader\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "            input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item() # sum loss for each batch\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches # avg loss over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "41f81c06-644c-4a8e-ba96-2eeb53bff9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583690219456\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad(): # disables gradient tracking for efficiency because we are not training yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device) # sure the data is loaded onto the same device as the LLM model.\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "030cae92-94ca-422c-8a89-ce885292e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader,\n",
    "    optimizer, device, num_epochs,\n",
    "    eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], [] # initialized list to track losses and tokens each\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs): # starts the training loop \n",
    "        model.train() # Starts the main training loop\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # resets loss gradients from prev batch iteration\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward() # calc loss gradients\n",
    "            optimizer.step() # updates model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % eval_freq == 0: # optional evaluation step\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, \"\n",
    "                    f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "                generate_and_print_sample( # prints a sample text after each epoch\n",
    "                    model, tokenizer, device, start_context\n",
    "                )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "468f774d-7f4d-478a-926b-fbd51332f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval() # dropout disabled during eval for stable, reproducible results\n",
    "    with torch.no_grad(): # disabled which is not required during evaluation, to reduce the computational overhead\n",
    "        train_loss = calc_loss_loader(\n",
    "        train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "        val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a023d176-348e-44c7-b2e3-9edb5f6b37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "42ac3f1e-8067-4ee1-9d22-1a2cab9139f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the the,,,,,,,,,,,,,,,,,,,\n",
      "Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339\n",
      "Every effort moves you, the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616\n",
      "Every effort moves you, and, and,, and, and,,, and, and, and,, and,,,, and, and,, and,,,, and,, and,,, and, and,,, and, and\n",
      "Ep 3 (Step 000020): Train loss 5.726, Val loss 6.600\n",
      "Every effort moves you, and I had\"                                             \n",
      "Ep 3 (Step 000025): Train loss 5.201, Val loss 6.348\n",
      "Every effort moves you his! Gisburn. Gisburn to the picture.   \"I        \"I. \"I the of the--and it.  \"I was--III was the\n",
      "Ep 4 (Step 000030): Train loss 4.417, Val loss 6.278\n",
      "Every effort moves you, and I had been the picture. \"I was aI was a\", I had been--and, I had been to the picture. I had the picture. I had the picture to the picture. \"I he was a\n",
      "Ep 4 (Step 000035): Train loss 4.069, Val loss 6.226\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Ep 5 (Step 000040): Train loss 3.732, Val loss 6.160\n",
      "Every effort moves you know                        He laughed again, and he had the donkey and he had been his pictures of his pictures and down the room, and he\n",
      "Ep 6 (Step 000045): Train loss 2.850, Val loss 6.179\n",
      "Every effort moves you know it was not that the picture--I had the fact with the last--his, and I was--and here are the Riv, and I had been his head to have.             \n",
      "Ep 6 (Step 000050): Train loss 2.427, Val loss 6.141\n",
      "Every effort moves you know,\" was one of the picture.  I turned to the last word.        He laughed again, I had back his head to the donkey.  \"I didn't--the. \"I\n",
      "Ep 7 (Step 000055): Train loss 2.104, Val loss 6.134\n",
      "Every effort moves you know,\" was not that I felt as it was the fact with a little a. Gisburn's past!     \"I was back the head to the donkey.  \"I looked up--because he had always his\n",
      "Ep 7 (Step 000060): Train loss 1.882, Val loss 6.233\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little a flash that he _rose of the fact, the cigars you like.\"  He placed them at my elbow and as I had been the--because he had always _\n",
      "Ep 8 (Step 000065): Train loss 1.320, Val loss 6.238\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no great surprise to me to have to see a smile behind his pictures.  \"Oh, I saw that, and down, and I was.\n",
      "Ep 8 (Step 000070): Train loss 0.985, Val loss 6.242\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000075): Train loss 0.717, Val loss 6.293\n",
      "Every effort moves you know,\" was one of my hostess was \"interesting\": on that point I could have given Miss Croft the fact, had been to the display of his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, as I turned, my eye fell on a small picture\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "model.parameters(), # method returns all trainable weight params of the model\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ead2d436-f2f2-4ed7-9438-16898350ad07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "    epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9bd47bcc-a2db-4613-a88f-c1856379a324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e7ab5005-7fee-4071-b9e6-9dff68b109ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "model=model,\n",
    "idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "max_new_tokens=25,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4e3b6919-21ac-4ea0-a24d-db982c079fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "\"closer\": 0,\n",
    "\"every\": 1,\n",
    "\"effort\": 2,\n",
    "\"forward\": 3,\n",
    "\"inches\": 4,\n",
    "\"moves\": 5,\n",
    "\"pizza\": 6,\n",
    "\"toward\": 7,\n",
    "\"you\": 8,\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "3e012bb6-cc41-41c1-9ae4-4ce948d781f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ffef7b84-d873-458b-a92e-a054b87e9f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([    0.0609,     0.0016,     0.0001,     0.5721,     0.0034,     0.0001,\n",
       "            0.0001,     0.3576,     0.0040])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])\n",
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5fda51d0-7cd0-404f-8f13-fe1a4a2d0012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "41eae456-33b1-45df-9b5b-89654efb6120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
    "                for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0aae763f-88ce-493e-af37-0956a1a2fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9eb717fc-02ef-45d1-8dd6-70e2963490d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1, 0.1, 5] # orginal, lower and higher confidence\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "                for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i],\n",
    "                bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44986be4-c3e0-49bf-ba72-f1c21ac9d317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f7d95-3a07-4ab6-bb96-07cd3e3161ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5f2618ba-f84b-463d-ba51-e95735ad17d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why top-k? using top-k , telling the model to select only from the top-k samples(highest probability).\n",
    "# rather than looking for the entire logits, to do this- select highest probabilities samples from the logits\n",
    "# and the rest will be masked (just like chapter-3 causal attention) \n",
    "# replaces with -inf before softmax after the rest of the logits would be assingned 0.00 nd the remaining probabilities sum up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "abbbedc9-a6c6-42b9-92e0-fdeee75c668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "258e61c8-841f-4e0e-a038-011c8261aa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1], # identifies logits less than the min in the top 3\n",
    "    input=torch.tensor(float('-inf')), # assigns -inf to lower logits\n",
    "    other=next_token_logits # retains the orginal logits for all othre tokens\n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c906d4ce-e13e-41ec-b9bc-6920a2024534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "076c3960-0f91-4233-a310-34cb11988bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "                temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "                logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "            \n",
    "        if top_k is not None: # filters logits with top-k sampling \n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0: # applies temperature scaling\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:     # Carries out greedy next-token selection as before when temperature scaling is disabled\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id: # stops generating early if end-of-seq token is encountered\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "16632dc1-63cb-4a0e-bc19-6ea03f3081dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves youlit terrace.\n",
      "\n",
      "\n",
      "\n",
      "\" he said deprecating laugh\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "8ec46ea3-ef9a-45db-a17c-595a538b5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "91fc4c12-5a7d-4dcc-aa22-d29338af93cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0f1a82b7-9cf5-4586-a54f-420e4ba11afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "fb2bde7b-a99a-48da-96b8-0a526ad07f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "efa88771-3626-405f-99b9-06050aa7dfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: unmatched '\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'tensorflow>=2.15.0' 'tqdm>=4.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a5c7f3c5-b751-4909-8c6f-61143441de09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x32c3202f0>)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "5b183f4d-d77f-4b1f-90f2-0b616ef3371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "1ff08b53-a388-4fbf-a90f-4ce9e926d488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3e163535-8777-4ed2-ac17-c5cee567e419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2a74137f-bb70-4724-b052-f60c01a6e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "08a3104c-420e-4eae-9d52-b71fcd4bd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "51768511-c31a-4188-88fe-fc69b33d6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "833c641a-d7d9-404b-a94f-915aacaaf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b4fc050e-8925-4d9d-bac9-e678d1b94ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "65253593-20e5-45dd-8349-50f2722261d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
    "                \"Right: {right.shape}\"\n",
    "        )\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9e912883-3869-4e8c-8b9b-0ead1d72a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def load_weights_into_gpt(gpt, params): # sets the models positional and token embedding weights to those specified in params\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])): # iterates over each transformer block in the model\n",
    "        q_w, k_w, v_w = np.split( # the np.split fn is used to dvde the attn and bias wgts into three eql parts for the q, k, and v compnts\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "        \n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"]) # the original GPT-2 model by OpenAI reused the token embedding weights\n",
    "                                                                     # in the output layer to reduce the total number of parameters,\n",
    "                                                                     # which is a concept known as weight tying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "05df92f2-86d9-4047-9302-0c101abac853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "89f4727f-b03b-469f-b216-889ab897a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward finding an ideal life. You don't have to accept your problems by trying to remedy them, because that would be foolish\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9a519d40-d19e-4401-8918-5044144feef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter- 06 fine tuning for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d9f1aeec-7680-454e-9ae7-7a8d0628827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(\n",
    "        url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download \" \"and extraction.\" )\n",
    "        return\n",
    "    with urllib.request.urlopen(url) as response: # dwnlds the file\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref: # unzip the file\n",
    "        zip_ref.extractall(extracted_path)\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path) # adds a .tsv file extention\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2ee0ec62-c7c7-41f4-8e44-aa7dd18b28b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will  b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will  b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\n",
    "data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "5796e4d6-6267-4448-a3ef-6d020604d8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts()) # ham - not spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "09abf0a0-f71a-4bd9-8e18-7df7d7812b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0] # counts the instances of \"spam\"\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample( \n",
    "        num_spam, random_state=123 # randomly smples \"ham\" instances to match the number of \"spam\" instances\n",
    "    )\n",
    "    balanced_df = pd.concat([\n",
    "        ham_subset, df[df[\"Label\"] == \"spam\"] # combine ham subset wth \"spam\"\n",
    "    ])\n",
    "    return balanced_df\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "533a5d51-f03d-4dd3-9960-5a489ac1465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the string class labels \"ham\" and \"spam\" into integer class labels 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "cc10bf6b-fc13-45bb-965f-a68ffe7a423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "bedc65dc-7fb5-4d82-be75-a922eb0bea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    \n",
    "    df = df.sample(\n",
    "        frac=1, random_state=123\n",
    "    ).reset_index(drop=True) # shuffles the entire df\n",
    "    train_end = int(len(df) * train_frac) # clacts split indices\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    train_df = df[:train_end] # splits the df\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "    \n",
    "train_df, validation_df, test_df = random_split( balanced_df, 0.7, 0.1 ) # test size is implied to be 0.2 as the remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "91db7547-9958-4808-969e-f53b80edf6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f7d5824b-4120-4877-b971-b363cfe98427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data loaders (like in text, each batch(chunk) for individual training unit\n",
    "# working with a spam dataset that contains text messages of vary-\n",
    "# ing lengths. To batch these messages as we did with the text chunks, we have two pri-\n",
    "# mary options:\n",
    "# -> Truncate all messages to the length of the shortest message in the dataset or batch (computationlly cheaper but significant info loss)\n",
    "# -> Pad all messages to the length of the longest message in the dataset or batch (gooing with this, padded to the longest msg in the seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "684ea62e-4143-4db0-885f-d82e38d8d88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})) # 50256 is padded to the shorter senteces to match with longer sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "2b4a86c1-d347-4573-bfd4-0962a41991b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.encoded_texts = [ # Pretokenizes texts\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "        \n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            \n",
    "            self.encoded_texts = [  # Truncates sequences if they are longer than max_length\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "            \n",
    "        self.encoded_texts = [ # Pads sequences to the longest sequence\n",
    "            encoded_text + [pad_token_id] *\n",
    "            (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        \n",
    "        # print(f\"[DEBUG] index={index}, label={label}, type={type(label)}\")\n",
    "\n",
    "        # try:\n",
    "        #     label_tensor = torch.tensor(int(label), dtype=torch.long)\n",
    "        # except Exception as e:\n",
    "        #     print(f\"[ERROR] Failed to convert label at index={index}\")\n",
    "        #     print(f\"Label value: {label} (type: {type(label)})\")\n",
    "        #     raise e\n",
    "            \n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "            return len(self.data)\n",
    "            \n",
    "    def _longest_encoded_length(self):\n",
    "            \n",
    "            max_length = 0\n",
    "            for encoded_text in self.encoded_texts:\n",
    "                encoded_length = len(encoded_text)\n",
    "                if encoded_length > max_length:\n",
    "                    max_length = encoded_length\n",
    "            return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "009c5e5b-d5ea-4511-9868-8e31c202a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "f0efdbf5-9203-44a9-9ece-f5c970a7036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "1a3bcc54-4b97-4d15-a235-5ad587a8b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f032f0c1-b3f0-4ac4-9b8c-906027e06859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "    \n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20994be7-437e-4228-a36d-b0cefe144ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "0b0556e0-1949-4f07-8b6e-ab2f5b822473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(balanced_df[\"Label\"].isna().sum())  # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "f79fa5c5-c1a5-4462-9024-6c35d6a86cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "b30ac12e-dee1-430e-bede-dd350ffbbe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f2a74589-e338-4ff9-a76d-25e3c953f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a model with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "86ebc8a2-1b3c-43df-921a-11b3ca73d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "fa26b57c-a404-4f5e-8914-4cb805296797",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "    \n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "b3a16254-1166-4cfa-a500-9033cd65f7b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "# from chapter05 import GPTModel, load_weights_into_gpt\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size, models_dir=\"gpt2\"\n",
    ")\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "f47a006c-d3c2-4db8-8b17-f747bc1c1f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "# from chapter04 import generate_text_simple\n",
    "# from chapter05 import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "12387ef3-600a-4b94-9938-d354cf96e392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "cde86221-7a13-476a-9fbe-53218cc4070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "22a4f392-8a1a-43a5-a626-2d249ecc6312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "afc43140-51a1-4f2e-be50-44de0de9adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b6fcd163-4e37-46fa-b213-b8755fde5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123) # addding a classification layer\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG[\"emb_dim\"],\n",
    "    out_features=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "953f5cdd-df15-4d51-bca6-4d255b695b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.trf_blocks[-1].parameters(): # model.tfr_blocks[-1] instead of only last transfomrer layer set to all layers(exercise)\n",
    "    param.requires_grad = True\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e031e601-e3ab-472a-8950-85495b381a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "994ceee8-716b-4d85-8978-efa1888369d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "629ac945-fb70-48c9-ae48-8af2fd8d6d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "39a1eee2-4ba4-4019-9b5c-7027571ba39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the classification loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "5e9eaec5-4f11-4f06-87a3-371189849378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "e19c2117-2a64-4de0-8972-360414013fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "3181c5e4-4d9e-400e-8529-565c180ae2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ee9419b6-c1c1-4e0b-bc7f-8a04eeb8c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    \n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    \n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :] # logits of last output token\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (\n",
    "                (predicted_labels == target_batch).sum().item()\n",
    "            )\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "31ea2730-46d2-40ff-b7e8-7bcc9d0a4c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, model, device, num_batches=10\n",
    ")\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, model, device, num_batches=10\n",
    ")\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, model, device, num_batches=10\n",
    ")\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "3d029914-725e-48c8-92e8-e294dffae130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :] # logits of last o/p token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b298446d-0c2f-46f6-a340-a6ee935db7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None): # calcg the classification loss\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "312f325f-d16d-49c2-9532-0a709471f865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.453\n",
      "Validation loss: 2.583\n",
      "Test loss: 2.322\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # # Disables gradient tracking for efficiency because we are not training yet\n",
    "    train_loss = calc_loss_loader(\n",
    "        train_loader, model, device, num_batches=5 \n",
    "    )\n",
    "val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "0e09766f-96fa-4b35-926e-5394e2be4b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a training function to fine-tune the model, which means\n",
    "# adjusting the model to minimize the training set loss. Minimizing the training set loss\n",
    "# will help increase the classification accuracy, which is the overall goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "0cf5c983-791e-4c95-aab8-5ab6e5d7dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning the model on supervised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "fd940bca-0881-4ffb-8b19-3f9bac05d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_simple( model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], [] # to track loses and examples seen\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs): # main training loop\n",
    "        model.train() # sets model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader: \n",
    "            optimizer.zero_grad() # # resets the loss grad from the previous batch iteration\n",
    "            loss = calc_loss_batch(\n",
    "            input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward() # calc loss gradient\n",
    "            optimizer.step() # updates models weights using loss gradients\n",
    "            examples_seen += input_batch.shape[0] # new: tracks examples instead of tokens\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0: # optional eval step\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, \"\n",
    "                    f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "                \n",
    "        train_accuracy = calc_accuracy_loader( # Calculates accuracy after each epoch\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_accuracy = calc_accuracy_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        \n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "3f3cfe0b-1ca7-449f-ad9f-135e0e81a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "cbdd5924-5083-4bb6-8a65-4ba8c067fda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392\n",
      "Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637\n",
      "Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557\n",
      "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
      "Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489\n",
      "Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397\n",
      "Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353\n",
      "Training accuracy: 82.50% | Validation accuracy: 85.00%\n",
      "Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320\n",
      "Ep 3 (Step 000350): Train loss 0.340, Val loss 0.306\n",
      "Training accuracy: 90.00% | Validation accuracy: 90.00%\n",
      "Ep 4 (Step 000400): Train loss 0.136, Val loss 0.200\n",
      "Ep 4 (Step 000450): Train loss 0.153, Val loss 0.132\n",
      "Ep 4 (Step 000500): Train loss 0.222, Val loss 0.137\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550): Train loss 0.207, Val loss 0.143\n",
      "Ep 5 (Step 000600): Train loss 0.083, Val loss 0.074\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 5.92 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
    "    train_classifier_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=num_epochs, eval_freq=50,\n",
    "        eval_iter=5\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "64ea70e1-a368-4d60-b438-60602c67353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with all trf_blocks.\n",
    "# Ep 1 (Step 000000): Train loss 2.881, Val loss 2.593\n",
    "# Ep 1 (Step 000050): Train loss 0.302, Val loss 0.195\n",
    "# Ep 1 (Step 000100): Train loss 0.120, Val loss 0.522\n",
    "# Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
    "# Ep 2 (Step 000150): Train loss 0.174, Val loss 0.063\n",
    "# Ep 2 (Step 000200): Train loss 0.009, Val loss 0.086\n",
    "# Ep 2 (Step 000250): Train loss 0.070, Val loss 0.113\n",
    "# Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
    "# Ep 3 (Step 000300): Train loss 0.010, Val loss 0.120\n",
    "# Ep 3 (Step 000350): Train loss 0.016, Val loss 0.055\n",
    "# Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
    "# Ep 4 (Step 000400): Train loss 0.007, Val loss 0.049\n",
    "# Ep 4 (Step 000450): Train loss 0.019, Val loss 0.098\n",
    "# Ep 4 (Step 000500): Train loss 0.004, Val loss 0.051\n",
    "# Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
    "# Ep 5 (Step 000550): Train loss 0.005, Val loss 0.039\n",
    "# Ep 5 (Step 000600): Train loss 0.005, Val loss 0.078\n",
    "# Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
    "# Training completed in 13.97 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "6888dca6-edb6-4db4-8563-772ca79f1f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\") # plots training and validation loss against epoch\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_values, linestyle=\"-.\",\n",
    "        label=f\"Validation {label}\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = ax1.twiny() # creates a second x-axis for examples seen\n",
    "    ax2.plot(examples_seen, train_values, alpha=0) # invisible plot for alingning ticks\n",
    "    ax2.set_xlabel(\"Examples seen\") \n",
    "\n",
    "    fig.tight_layout() # adjusts layout to make room\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "417e1059-6164-47f4-9ef5-0acbc75e0695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "plot_values(\n",
    "epochs_tensor, examples_seen_tensor, train_accs, val_accs,\n",
    "label=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e00e8f26-8403-4a34-923b-9b13f33faddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.21%\n",
      "Validation accuracy: 97.32%\n",
      "Test accuracy: 95.67%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "3fca368a-f4a0-4dc9-a999-9e06d45abe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the LLM as a spam classifier - using the model to classify new texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "8455295c-527c-4b05-af28-7900d9807ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "\n",
    "    input_ids = tokenizer.encode(text) # prepare inputs to the model\n",
    "    supported_context_length = model.pos_emb.weight.shape[1]\n",
    "\n",
    "    input_ids = input_ids[:min( # truncates seqs if they are too long\n",
    "    max_length, supported_context_length\n",
    "    )]\n",
    "\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids)) # pads seqs to the longest seq\n",
    "\n",
    "    input_tensor = torch.tensor( # adds batch dim\n",
    "        input_ids, device=device\n",
    "    ).unsqueeze(0) \n",
    "\n",
    "    with torch.no_grad(): # models inference without gradient tracking\n",
    "        logits = model(input_tensor)[:, -1, :] # logits of the last o/p token\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\" # last classified ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4a706a5e-8825-4845-bd3f-bc39563d2bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0a911697-b376-406f-9ca1-03eb72c68746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "a8fafa37-be50-44ef-b9fd-1e131a20ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "884fa449-50d5-40ff-bec1-ba0c444a1230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"review_classifier.pth\", map_location=device)\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863977e1-6774-4096-844a-62d4b24269ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
