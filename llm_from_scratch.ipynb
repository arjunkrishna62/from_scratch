{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c2757bea-ce81-4bac-a106-6765790401d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter 2 - text prerpocessing, tokenization, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "60cdd03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x127f5aae0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "\"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d165cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cf28145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dc41c52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "152ae936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f581d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n",
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "print(result)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e286bead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying it to the dataset\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2ba15217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'HAD',\n",
       " 'always',\n",
       " 'thought',\n",
       " 'Jack',\n",
       " 'Gisburn',\n",
       " 'rather',\n",
       " 'a',\n",
       " 'cheap',\n",
       " 'genius']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ca15d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "933fffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2674fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text) # to numerical values \n",
    "        preprocessed = [\n",
    "        item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids): # back to natural language\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dd219693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e8c785a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "368ed6f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "Cell \u001b[0;32mIn[91], line 10\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      6\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text) \u001b[38;5;66;03m# to numerical values \u001b[39;00m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      9\u001b[0m ]\n\u001b[0;32m---> 10\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text)) # error due to the text is not in training set. The word 'hello' not used in the short story 'The verdict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a1616306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding special context tokens :- that is we add <|unk|> to new or unknown words while <|endoftext|> for sentence completionb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9ec28495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "# modifying the vocabulary \n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8ef4f2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "21849a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "        item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "        else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d4bae2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "90b424fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text)) # 1131 = <|unk|> and 1130 = <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "13a9b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "65adf89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "66d5d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tik_tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "99e09bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "\"of someunknownPlace.\"\n",
    ")\n",
    "integers = tik_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d0835db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tik_tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6afbf9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959, 13]\n"
     ]
    }
   ],
   "source": [
    "unk = \"Akwirw ier.\"\n",
    "print(tik_tokenizer.encode(unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ac9c48a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akwirw ier.\n"
     ]
    }
   ],
   "source": [
    "print(tik_tokenizer.decode(tik_tokenizer.encode(unk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "095d7f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "# now tokenize the short story dataset using the BPE tokenizer\n",
    "enc_text = tik_tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5f800643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5095"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "len(enc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4fef225f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y: [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "263a9cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6abfba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tik_tokenizer.decode(context), \"---->\", tik_tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "56e94067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tik_tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4384763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "stride=128, shuffle=True, drop_last=True,\n",
    "num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            drop_last=drop_last,\n",
    "                            num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7ea39b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c47f5680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "second_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fb95ceda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4,\n",
    "    shuffle=False\n",
    "    )\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "22b9c570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# token IDs into token embedding vectors\n",
    "\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bef3596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3]))) # embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "40931bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b16f42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usefull embdng sizes and encode the input tokens into a\n",
    "# 256-dimensional vector representation, which is smaller than what the original GPT-3\n",
    "# model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable\n",
    "# for experimentation. furthermore, we assume that the token IDs were created by the\n",
    "# BPE tokenizer we implemented earlier, which has a vocabulary size of 50,257:\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "655189de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "raw_text, batch_size=8, max_length=max_length,\n",
    "stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3ff4e355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f503675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bd6fdb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3d9c3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter 3 -  self-attention intro and MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "076db777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # your     (x^1)\n",
    " [0.55, 0.87, 0.66], # journey  (x^2)\n",
    " [0.55, 0.87, 0.66], # starts   (x^3)\n",
    " [0.22, 0.58, 0.33], # with     (x^4)\n",
    " [0.77, 0.25, 0.10], # one      (x^5)\n",
    " [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b6a7adb5-77ff-415f-9861-b679ca1a80a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8eb9d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] \n",
    "d_in = inputs.shape[1] # input embedding size , d = 3\n",
    "d_out = 2 # output embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e2b1f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6043e426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2961, 0.5166],\n",
       "        [0.2517, 0.6886],\n",
       "        [0.0740, 0.8665]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4ca54d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "157fb950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2c973cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3669, 0.7646],\n",
       "        [0.4433, 1.1419],\n",
       "        [0.4433, 1.1419],\n",
       "        [0.2408, 0.6706],\n",
       "        [0.1827, 0.3292],\n",
       "        [0.3275, 0.9642]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3f4b96fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22) # unnormalized attention score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "06191791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8524, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # computaton to all attention scores\n",
    "print(attn_scores_2) # 2nd element matches we computed prev (attn_scores_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3e1e71ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1490, 0.2249, 0.2249, 0.1302, 0.0900, 0.1808])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1] \n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1) # qk^T / root_dk(dim of the key matrix)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "203e02dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3082, 0.8267])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e2697e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compact self-attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "06eee1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax( attn_scores / keys.shape[-1]**0.5, dim=-1 )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7b6b9754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3015, 0.8104],\n",
      "        [0.3082, 0.8267],\n",
      "        [0.3082, 0.8267],\n",
      "        [0.2965, 0.7986],\n",
      "        [0.2944, 0.7936],\n",
      "        [0.3009, 0.8091]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs)) # inputs contains 6 embedding vectors , results in a matrix storing 6 context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f250d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention using linear layer.\n",
    "# instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear\n",
    "# has an optimized weight initialization scheme, contributing to more stable and\n",
    "# effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1eadaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax( attn_scores / keys.shape[-1]**0.5, dim=-1 )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8bd18043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5340, -0.1049],\n",
      "        [-0.5326, -0.1078],\n",
      "        [-0.5326, -0.1078],\n",
      "        [-0.5300, -0.1074],\n",
      "        [-0.5313, -0.1064],\n",
      "        [-0.5302, -0.1079]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs)) # SelfAttention_v1 and SelfAttention_v2 give different outputs because\n",
    "                     # they use different initial weights for the weight matrices since nn.Linear uses a more\n",
    "                     # sophisticated weight initialization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5e5fd9de-a9c2-407b-9372-5b82f24d6398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1716, 0.1762, 0.1762, 0.1555, 0.1626, 0.1579],\n",
      "        [0.1635, 0.1749, 0.1749, 0.1611, 0.1604, 0.1651],\n",
      "        [0.1635, 0.1749, 0.1749, 0.1611, 0.1604, 0.1651],\n",
      "        [0.1636, 0.1703, 0.1703, 0.1651, 0.1632, 0.1674],\n",
      "        [0.1667, 0.1722, 0.1722, 0.1617, 0.1633, 0.1639],\n",
      "        [0.1624, 0.1708, 0.1708, 0.1654, 0.1624, 0.1681]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "                                # Reuses the query and key weight matrices\n",
    "                                # of the SelfAttention_v2 object from the\n",
    "                                # previous section for convenience\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c7ed6723-9995-4b00-8bf6-1fdb6b93a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length)) #tril fn to create a mask where the values above the diagonal are zero\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a07c154e-7a48-44fc-b872-d57ad78c0494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1716, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1635, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1635, 0.1749, 0.1749, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1636, 0.1703, 0.1703, 0.1651, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1722, 0.1722, 0.1617, 0.1633, 0.0000],\n",
       "        [0.1624, 0.1708, 0.1708, 0.1654, 0.1624, 0.1681]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f89644d0-f015-4b18-b5a8-8ac58f24bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3186, 0.3407, 0.3407, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2444, 0.2544, 0.2544, 0.2467, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2060, 0.1934, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1708, 0.1708, 0.1654, 0.1624, 0.1681]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm) # renormalize the attention weights to sum up to 1 again in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "217f7b26-d4b4-46aa-824b-55675724fd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1716],\n",
       "        [0.3384],\n",
       "        [0.5133],\n",
       "        [0.6694],\n",
       "        [0.8361],\n",
       "        [1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d123de43-a8f5-444e-9847-281df10b8535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.2602, 0.2602,   -inf,   -inf,   -inf],\n",
      "        [0.0510, 0.1080, 0.1080, 0.0643,   -inf,   -inf],\n",
      "        [0.1415, 0.1875, 0.1875, 0.0987, 0.1121,   -inf],\n",
      "        [0.0476, 0.1192, 0.1192, 0.0731, 0.0477, 0.0966]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "91e397e7-4fa7-40ca-8530-2d14edfd57fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3186, 0.3407, 0.3407, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2444, 0.2544, 0.2544, 0.2467, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2060, 0.1934, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1708, 0.1708, 0.1654, 0.1624, 0.1681]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "43afff34-938f-4468-93f7-4efbee9c25ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1a2dc2a6-0cc5-4f13-89d1-5667e20193a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6373, 0.6814, 0.6814, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5089, 0.5089, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4119, 0.0000, 0.3869, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3417, 0.3417, 0.3307, 0.3248, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e4d7304f-762a-48a3-af6f-9b4af931589e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0466e467-dbd7-4d70-8f80-33025cd5dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer( # buffers are used automatically move to the apropriate device (cpu or gpu) along with the model\n",
    "        'mask',\n",
    "        torch.triu(torch.ones(context_length, context_length),\n",
    "        diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c6f334bc-5c8e-4028-9bdd-d1ef0c5a3c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5845e9e3-810b-4c0a-b532-ee12f357a6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6306, -0.0630],\n",
       "         [-0.5679, -0.0840],\n",
       "         [-0.5529, -0.0979],\n",
       "         [-0.5302, -0.1079]],\n",
       "\n",
       "        [[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6306, -0.0630],\n",
       "         [-0.5679, -0.0840],\n",
       "         [-0.5529, -0.0979],\n",
       "         [-0.5302, -0.1079]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "675d9900-5a95-4964-bf21-8540f21532f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "        dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(\n",
    "                d_in, d_out, context_length, dropout, qkv_bias\n",
    "            )\n",
    "            for _ in range(num_heads)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1) # processing sequentially not simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fb17d73c-275e-47d8-8d50-9d2509ef6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "67f2ab62-9815-4e46-8e1d-e05788b88e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6306, -0.0630,  0.6206,  0.3875],\n",
      "         [-0.5679, -0.0840,  0.5479,  0.3597],\n",
      "         [-0.5529, -0.0979,  0.5322,  0.3434],\n",
      "         [-0.5302, -0.1079,  0.5077,  0.3499]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6306, -0.0630,  0.6206,  0.3875],\n",
      "         [-0.5679, -0.0840,  0.5479,  0.3597],\n",
      "         [-0.5529, -0.0979,  0.5322,  0.3434],\n",
      "         [-0.5302, -0.1079,  0.5077,  0.3499]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "da2dd240-ead2-4617-b3c5-f7ca33ec8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor is two because we have 2 input texts  (the input texts are duplicated, which is why the context vectors are exactly the same for those)\n",
    "# second dim refers to the 6 tokens in each input\n",
    "# third dim refers to the 4 dim embedding of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "914f4428-55e0-48d1-b7d8-59396aff2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "        context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # reduces the projection dim to match the desired output dim\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # uses a Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                diagonal=1)\n",
    "        )\n",
    "        \n",
    "    # tensor shape: (b, num_tokens, d_out)\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # we implicitly split the matrix by adding a num_heads dimension.\n",
    "        #then we unroll the last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim).\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(\n",
    "            b, num_tokens, self.num_heads, self.head_dim\n",
    "        )\n",
    "        #transposes from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2) \n",
    "        values = values.transpose(1, 2)\n",
    "    \n",
    "        attn_scores = queries @ keys.transpose(2, 3) # computes dot for each head\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # masks truncated to the no of tokens\n",
    "    \n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) # uses mask to fill attn_scores\n",
    "    \n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "    \n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # tensor shape: (b, num_tokens, n_heads, head_dim)\n",
    "    \n",
    "        # Combines heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "        b, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec) # adds an optional linear projection\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "23dd97cd-c7d2-43f4-a3d1-cc059b04fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "[0.8993, 0.0390, 0.9268, 0.7388],\n",
    "[0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "[[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "[0.4066, 0.2318, 0.4545, 0.9737],\n",
    "[0.4606, 0.5159, 0.4220, 0.5786]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f7674f3a-06a9-455d-9247-6c0d74490949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "print(a @ a.transpose(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "68f97b6e-b2d4-4c4b-9e7c-c9adbc1e963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "aaaf6fde-5c54-4218-bd67-b9af137c8422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "47b51e7d-0abd-49cf-b23a-e1c579fb173e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2857, 0.3590],\n",
      "         [0.2694, 0.3872],\n",
      "         [0.2640, 0.3926],\n",
      "         [0.2576, 0.4027]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2857, 0.3590],\n",
      "         [0.2694, 0.3872],\n",
      "         [0.2640, 0.3926],\n",
      "         [0.2576, 0.4027]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ec7954f1-0ccd-45c0-9c0c-db68e1ea620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter 4 - LLM architecture :-  implementing gpt-2 , transformers 124m (also diff block as exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8e9d53e8-2861-47eb-bc5e-042e07d45c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257,     # Vocabulary size\n",
    "\"context_length\": 1024,  # Context length\n",
    "\"emb_dim\": 768,          # Embedding dimension\n",
    "\"n_heads\": 12,           # Number of attention heads\n",
    "\"n_layers\": 12,          # Number of layers\n",
    "\"drop_rate\": 0.1,        # Dropout rate\n",
    "\"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3e7ba913-663b-4c75-9360-2adea3b5eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg)\n",
    "            for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "        torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6c4faf8a-c314-4513-8b83-a40fad379fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a39d5c85-5114-44f4-bfe2-e7721089b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3e7759b6-a2df-49da-acbc-656dbec2d582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
       "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
       "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
       "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
       "\n",
       "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
       "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
       "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
       "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch) # logits are unormalized model's pred before an actvn fn is applied , these values can range from -ve to +ve infinity\n",
    "print(\"Output shape:\", logits.shape) # and represent the model's confidence in assigning an input to a particular class.\n",
    "logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b0984587-c3a0-4265-974d-a1542896e6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
       "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c39e5ef9-4493-4f33-b993-9bde7e6199eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
       "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a446f493-924a-4562-8ccb-7a325e14539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e377b37f-8ccf-4f12-9823-1c98855a3b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6e78413d-5d53-4502-8a1b-b975de32c174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False) \n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "29cbe8ec-7400-45b3-8f1f-39f80262737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "9f45353e-31dd-414b-82be-2ee2683c30b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "44d8c09a-fe44-40b5-8e3e-c5640f4ec872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7927c57e-edaa-4e4c-9133-3e7b0f85ce9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "# plt.tight_lay\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bf15cc53-e431-443c-b95b-41d4637c23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The smoothness of GELU can lead to better optimization properties during training,\n",
    "# as it allows for more nuanced adjustments to the model’s parameters. In contrast,\n",
    "# ReLU has a sharp corner at zero , which can sometimes make opti-mization harder, \n",
    "# especially in networks that are very deep or have complex architectures.\n",
    "# Moreover, unlike ReLU, which outputs zero for any negative input, GELU\n",
    "# allows for a small, non-zero output for negative values. This characteristic means that\n",
    "# during the training process, neurons that receive negative input can still contribute to\n",
    "# the learning process, albeit to a lesser extent than positive inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e0f4c6f5-92f4-4a65-9463-6f37b71d76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c7b2fc53-be3a-4f01-9fd1-7a5fd201299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b1b76820-7c62-4680-80a8-ab66002a5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut Connections or Skip Conncetions or Residual Connections\n",
    "# as the layer progress there's a high chance of problem like vanishing gradient.\n",
    "# vanshing gradient :- vanishing gradient problem refers to the issue where gradients\n",
    "#     (which guide weight updates during training) become progressively smaller as they\n",
    "#     propagate backward through the layers, making it difficult to effectively train earlier\n",
    "#     layers.\n",
    "# to prevent vanishing gradiant prblem  the soln is  skip or residual conncetions :-\n",
    "#         creates alternative or shotcut path for grdient flow  thruogh the network by skipping one or more layers,\n",
    "#         which is achieved by adding output of one layer to the output of a later layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3c3dbe90-0a74-4077-82b0-d49381f11107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "        nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]),\n",
    "        GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]),\n",
    "        GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]),\n",
    "        GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]),\n",
    "        GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]),\n",
    "        GELU())\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x) # compute the output of the current layer\n",
    "            if self.use_shortcut and x.shape == layer_output.shape: # check if shortcuts can be applied\n",
    "                x = x + layer_output \n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "079a9ae0-d5e5-4d87-977d-3f36db3e1199",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specifies random seeds for initial weights for reproducibility\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d40838b7-ddfc-4a2a-a838-d029f84c3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x) # fwd pass\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target) # calculates loss based on how close the target and output are\n",
    "    loss.backward() # backward pass to calculate the gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "2d7b6d71-3d98-4929-bab5-af6ceafacfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
      "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1b36521f-694d-4f53-9630-670d5b1d385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the gradient becomes smaller when we progress from the last layer (vanishing gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "75b160e8-82cd-492c-b8d2-0668467ecc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True # applying skip connections\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "fc63b10e-2815-41fb-991f-366d3c0b8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        shortcut = x # shortcut conn for attention block\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # add the orginal input back\n",
    "\n",
    "        shortcut = x # shortcut conn for ff block\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # adds the orginal back\n",
    "        return x\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9bc3d7ca-9958-4f27-958a-9098c995a35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) # smaple i/p shape [batch_size, num_tokens, num_emb]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b77e0173-e03b-4726-b267-f7900e4fb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb( \n",
    "        torch.arange(seq_len, device=in_idx.device) # device setting will allow us to train the model on a CPU or GPU, depending on which device the input data sits on.\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9b97256e-7839-4fa6-85f2-38aaf920fd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4223, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "c7fe94ce-5818-4514-8726-ccd3cfc12ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters()) # analyzing size using numel() - number of elements, we can collect\n",
    "print(f\"Total number of parameters: {total_params:,}\") # total number of parameters in the model's parameter tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "693db8f9-7654-4865-b260-047c036b5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why 163m instead of 124m ? - reason is a concept called \"weight tying\" which was used in the original GPT-2 architecture.\n",
    "# It means that the original GPT-2 architecture reuses the weights from the tokn embdng layer in its output layer.\n",
    "# To understand better, let’s take a look at the shapes of the token emdng layer and linear ouput layer that we initialzed\n",
    "# on the model via the GPTModel earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "dbe101d4-9bd7-4949-acc3-04905dbd182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8fdf2d24-06b4-4436-9f21-17f48a852366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token embedding and output layers are very large due to the number of rows for\n",
    "# the 50,257 in the tokenizer’s vocabulary. Let’s remove the output layer parameter\n",
    "# count from the total GPT-2 model count according to the weight tying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2490e3f1-d129-455f-a747-f310f1d7b46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "total_params - sum(p.numel()\n",
    "for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {total_params_gpt2:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e612687a-808b-4e68-a730-a53b1e3e62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parameters in feed forward and attention modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0115dcda-0ea1-4b2f-9fca-dff6b16c1275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feed fwd parameteres: 4,722,432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FeedForward(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn_param = sum(p.numel() for p in ffn.parameters())\n",
    "print(f\"feed fwd parameteres: {ffn_param:,}\")\n",
    "ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e01cb27d-5028-41cb-8831-d20effab5f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttn parameters: 24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (W_query): Linear(in_features=3, out_features=2, bias=False)\n",
       "  (W_key): Linear(in_features=3, out_features=2, bias=False)\n",
       "  (W_value): Linear(in_features=3, out_features=2, bias=False)\n",
       "  (out_proj): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_param = sum(p.numel() for p in mha.parameters())\n",
    "print(f\"MultiHeadAttn parameters: {mha_param:,}\")\n",
    "mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a8982014-6b86-4af3-bb01-ca0551d7d879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4718592"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(768 * 3072) * 2 # ffnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "16109225-ae43-4b73-a3b0-202ffa6d5634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596736"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((3 * 768) * 3) + (768 * 768) # mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "650fbe76-6315-4880-80a4-2403ae4341ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# lastly compute the memory req for 163m\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "233c722a-e016-4bc9-9cbe-d61587771ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing larger gpt models, first gpt_medium with 24 tfr blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e009f7aa-08bd-4dbb-a0ac-2c9ce1cfc773",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_MEDIUM = {\n",
    "\"vocab_size\": 50257,     # Vocabulary size\n",
    "\"context_length\": 1024,  # Context length\n",
    "\"emb_dim\": 1024,          # Embedding dimension\n",
    "\"n_heads\": 16,           # Number of attention heads\n",
    "\"n_layers\": 24,          # Number of layers\n",
    "\"drop_rate\": 0.1,        # Dropout rate\n",
    "\"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8a97101f-bf3a-45aa-8b6d-296c1b82eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb( \n",
    "        torch.arange(seq_len, device=in_idx.device) # device setting will allow us to train the model on a CPU or GPU, depending on which device the input data sits on.\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "94ae36e4-01df-4e40-aca1-fbf19764d43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5553,  0.4949,  0.3092,  ..., -0.2539, -0.3032,  0.2971],\n",
       "         [ 0.3024, -0.0328, -0.5984,  ..., -0.6252, -0.5445, -0.9409],\n",
       "         [ 0.4388,  0.3456, -0.2936,  ...,  0.5144,  0.0907,  0.0242],\n",
       "         [ 0.1325,  0.6145, -0.8940,  ...,  0.3889,  0.0279, -0.5072]],\n",
       "\n",
       "        [[ 1.0220,  0.3779,  0.8361,  ...,  0.1372, -0.2403,  0.8152],\n",
       "         [ 0.0192,  0.3544, -0.5428,  ..., -0.0214,  0.0261, -0.4027],\n",
       "         [ 0.2641,  0.1448, -0.3102,  ..., -0.0764, -0.4402,  0.2604],\n",
       "         [ 0.6284,  1.2054, -0.6994,  ...,  0.0841, -0.3732, -0.1643]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "gpt_medium = GPTModel(GPT_CONFIG_MEDIUM)\n",
    "medium_out = gpt_medium(batch)\n",
    "print(medium_out.shape)\n",
    "medium_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b13839b4-1ce5-44f7-b46f-649ae432a397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406,212,608\n"
     ]
    }
   ],
   "source": [
    "ttl_gpt_medium_params = sum(p.numel() for p in gpt_medium.parameters()) # small model has 163,009,536 , 24 block has 406m\n",
    "print(f\"{ttl_gpt_medium_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4ee869de-c323-4a21-9551-e0439754036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 111,546,368\n"
     ]
    }
   ],
   "source": [
    "ttl_gpt_medium_params = (\n",
    "total_params - sum(p.numel()\n",
    "for p in gpt_medium.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {ttl_gpt_medium_params:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "81e615c9-fdef-4d0e-b7ca-7680a3184507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt_large  with 36 tfr blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "29703235-bb86-42ad-9529-990a788ff598",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_LARGE = {\n",
    "\"vocab_size\": 50257,     # Vocabulary size\n",
    "\"context_length\": 1024,  # Context length\n",
    "\"emb_dim\": 1280,          # Embedding dimension\n",
    "\"n_heads\": 20,           # Number of attention heads\n",
    "\"n_layers\": 36,          # Number of layers\n",
    "\"drop_rate\": 0.1,        # Dropout rate\n",
    "\"qkv_bias\": False        # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb( \n",
    "        torch.arange(seq_len, device=in_idx.device) # device setting will allow us to train the model on a CPU or GPU, depending on which device the input data sits on.\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "04a6f4e5-bc6e-4cf7-ac75-f2edca7c5c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[    -0.1292,     -0.3847,      0.2248,  ...,      0.5369,\n",
       "               0.3691,     -0.7619],\n",
       "         [     0.4239,      0.2338,     -1.0530,  ...,     -0.0855,\n",
       "              -0.6005,     -0.4725],\n",
       "         [     0.2074,     -0.6801,     -0.0454,  ...,     -1.2456,\n",
       "              -0.8294,     -0.0187],\n",
       "         [     0.4618,     -0.2833,     -0.4043,  ...,      0.0392,\n",
       "              -0.5204,     -0.2321]],\n",
       "\n",
       "        [[    -0.6438,      0.1008,      0.2805,  ...,      0.5632,\n",
       "               0.7455,     -0.3469],\n",
       "         [     0.5257,     -0.0004,     -0.1389,  ...,     -0.3714,\n",
       "               0.3727,      0.1004],\n",
       "         [     0.3873,     -0.7259,     -0.1061,  ...,     -1.1120,\n",
       "              -0.4745,      0.2956],\n",
       "         [     0.2372,     -0.0430,      0.1782,  ...,     -0.4974,\n",
       "              -0.2305,      0.2401]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "gpt_large = GPTModel(GPT_CONFIG_LARGE)\n",
    "large_out = gpt_large(batch)\n",
    "print(large_out.shape)\n",
    "large_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4ece34b0-2304-47fc-a119-7283ad72061b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838,220,800\n"
     ]
    }
   ],
   "source": [
    "ttl_gpt_large_params = sum(p.numel() for p in gpt_large.parameters()) \n",
    "print(f\"{ttl_gpt_large_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c6c1d232-9987-46eb-836c-c6c4980331f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 98,680,576\n"
     ]
    }
   ],
   "source": [
    "ttl_gpt_large_params = (\n",
    "total_params - sum(p.numel()\n",
    "for p in gpt_large.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {ttl_gpt_large_params:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "8bb153c3-9658-4482-8b66-bde4f8ce81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt xl with tfr blocks 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "963ec260-92d9-4bbb-ae8a-5433d1f578f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_XL = {\n",
    "\"vocab_size\": 50257,     # Vocabulary size\n",
    "\"context_length\": 1024,  # Context length\n",
    "\"emb_dim\": 1600,          # Embedding dimension\n",
    "\"n_heads\": 25,           # Number of attention heads\n",
    "\"n_layers\": 48,          # Number of layers\n",
    "\"drop_rate\": 0.1,        # Dropout rate\n",
    "\"qkv_bias\": False        # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb( \n",
    "        torch.arange(seq_len, device=in_idx.device) # device setting will allow us to train the model on a CPU or GPU, depending on which device the input data sits on.\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2a596413-ceaa-48ff-821b-11d395c2382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1291,  0.3885,  0.1057,  ...,  0.0180,  0.2309, -0.3901],\n",
       "         [-0.2870, -0.0277, -0.3349,  ..., -0.0724,  0.1030, -0.3322],\n",
       "         [-0.6001, -0.2720,  0.3672,  ...,  0.0804,  0.6928,  0.0880],\n",
       "         [-1.3517, -0.3163,  0.2312,  ...,  0.3396, -0.0351,  0.3853]],\n",
       "\n",
       "        [[-0.1893,  0.3246, -0.1917,  ..., -0.2170,  0.2229, -0.6889],\n",
       "         [ 0.0024, -0.1743, -0.2006,  ...,  0.0684, -0.9490, -0.8778],\n",
       "         [-0.0397,  0.0353,  0.2041,  ..., -0.3985,  0.1638, -0.2869],\n",
       "         [-0.7979,  0.2167, -0.0873,  ...,  0.9055, -0.4025, -0.0753]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "gpt_xl = GPTModel(GPT_CONFIG_XL)\n",
    "xl_out = gpt_xl(batch)\n",
    "print(xl_out.shape)\n",
    "xl_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a5a1ecd0-0d0d-4e5c-af29-37eaf09be860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,637,792,000\n"
     ]
    }
   ],
   "source": [
    "ttl_gpt_large_params = sum(p.numel() for p in gpt_xl.parameters()) \n",
    "print(f\"{ttl_gpt_large_params:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "cbdd5fd4-4686-4f69-aabf-de642a1f6eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 82,598,336\n"
     ]
    }
   ],
   "source": [
    "ttl_gpt_xl_params = (\n",
    "total_params - sum(p.numel()\n",
    "for p in gpt_xl.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {ttl_gpt_xl_params:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "cee9a3e1-7298-4fe5-93b6-925ee1566acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, # dx is a (batch, n_tokens) array of indices in the current context.\n",
    "    max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:] # crops current context if exceeds the supported context size, if llm supports only 5 tokens, and the context is 10, then only the last 5 tokens are used as context\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :] # Focuses only on the last time step, so that (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        probas = torch.softmax(logits, dim=-1) # prob has shape (batch, vocab_size).\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True) # idx next has shape(batch, 1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # appends sampled text to the running sequence, where idx has shape(batch, n_tokens+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "26c4d0cf-757f-4bb7-894c-bcced50fa51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) # adds batch dim\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f746bf90-184f-495f-a44e-35ef4c5fc319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Disables dropout since we are not training the model\n",
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "f7cb41fd-1ac1-4e7e-8c3a-6cd7d7a0cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "5c3a9514-ef1d-4bb4-86b9-08cb6901163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter 5 - pretraining , model evaluation, taking pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5aca1d3f-e951-451a-9ab7-76279608ccb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# from chapter04 import GPTModel\n",
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257,\n",
    "\"context_length\": 256, # 1024 to 256\n",
    "\"emb_dim\": 768,\n",
    "\"n_heads\": 12,\n",
    "\"n_layers\": 12,\n",
    "\"drop_rate\": 0.1, # It’s possible and common to set dropout to 0.\n",
    "\"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "c3376f5b-cbb0-4aff-a591-adf252d7c510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "# from chapter04 import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # .unsqueeze(0) adds the batch dimension\n",
    "    return encoded_tensor\n",
    "    \n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # removes batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "model=model,\n",
    "idx=text_to_token_ids(start_context, tokenizer),\n",
    "max_new_tokens=10,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "8950ef74-c29a-4fca-854a-60414e2e28b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100], # [\"every effort moves\",\n",
    "[40, 1107, 588]]) # \"I really like\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "d3bcd6f1-ca08-46a2-b298-a63e215f7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([[3626, 6100, 345 ], # [\" effort moves you\",\n",
    "[1107, 588, 11311]]) # \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "45f5c3ca-944f-4a54-bf30-bf5fcb2d0063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # not training so dsable gradient tracking\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1) # prob of each token in vocabulary\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "136089e3-ccb0-4753-9b17-7d81c45de1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "7de8f2ab-f061-40b3-8d38-f5adfd567ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:   Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: \"f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "769ef434-7606-4155-9bd4-73c2cc768a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0001,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0001,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "397e098a-489b-4529-9c17-24cf1f9ce536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "188c67c0-0211-4719-b789-769477cbca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "27cee3a6-88ba-4bfa-8fe8-6b850d2bd53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "cd71ae2a-fad0-4cfa-aaa9-00332cdb7918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9bd9964e-65ed-4ef0-9aea-42e2768798df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "284cd5f2-f015-4bde-a334-c70ab4f69668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2697dea0-1705-4153-ae46-43354291e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "fdd0b4da-74d3-4227-a261-257293148c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "70bfa3c9-8952-4a12-9a44-215164cb2965",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "a29cadd2-9e2c-42ac-9caf-a231778a0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chapter02 import create_dataloader_v1\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "train_data,\n",
    "batch_size=2,\n",
    "max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "drop_last=True,\n",
    "shuffle=True,\n",
    "num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "val_data,\n",
    "batch_size=2,\n",
    "max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "drop_last=False,\n",
    "shuffle=False,\n",
    "num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "77490307-a8e1-4687-931e-b99aa97004f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "08313a03-eea9-4e5b-8fd6-b194041583b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device) # the transfer to a given device allows us to transfer the data to a GPU.\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "    logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d0d990ca-2ad4-46a1-a3f7-558924207bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader) # iteratives over all batches if no fixed num_batches is specified\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader)) # reduces the number of batches to match the total number of batches in the data loader if num_batches exceeds the number of batches in the data loader\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "            input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item() # sum loss for each batch\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches # avg loss over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "41f81c06-644c-4a8e-ba96-2eeb53bff9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583690219456\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad(): # disables gradient tracking for efficiency because we are not training yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device) # sure the data is loaded onto the same device as the LLM model.\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "030cae92-94ca-422c-8a89-ce885292e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader,\n",
    "    optimizer, device, num_epochs,\n",
    "    eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], [] # initialized list to track losses and tokens each\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs): # starts the training loop \n",
    "        model.train() # Starts the main training loop\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # resets loss gradients from prev batch iteration\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward() # calc loss gradients\n",
    "            optimizer.step() # updates model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % eval_freq == 0: # optional evaluation step\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, \"\n",
    "                    f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "                generate_and_print_sample( # prints a sample text after each epoch\n",
    "                    model, tokenizer, device, start_context\n",
    "                )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "468f774d-7f4d-478a-926b-fbd51332f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval() # dropout disabled during eval for stable, reproducible results\n",
    "    with torch.no_grad(): # disabled which is not required during evaluation, to reduce the computational overhead\n",
    "        train_loss = calc_loss_loader(\n",
    "        train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "        val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "a023d176-348e-44c7-b2e3-9edb5f6b37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "42ac3f1e-8067-4ee1-9d22-1a2cab9139f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the the,,,,,,,,,,,,,,,,,,,\n",
      "Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339\n",
      "Every effort moves you, the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616\n",
      "Every effort moves you, and, and,, and, and,,, and, and, and,, and,,,, and, and,, and,,,, and,, and,,, and, and,,, and, and\n",
      "Ep 3 (Step 000020): Train loss 5.726, Val loss 6.600\n",
      "Every effort moves you, and I had\"                                             \n",
      "Ep 3 (Step 000025): Train loss 5.201, Val loss 6.348\n",
      "Every effort moves you his! Gisburn. Gisburn to the picture.   \"I        \"I. \"I the of the--and it.  \"I was--III was the\n",
      "Ep 4 (Step 000030): Train loss 4.417, Val loss 6.278\n",
      "Every effort moves you, and I had been the picture. \"I was aI was a\", I had been--and, I had been to the picture. I had the picture. I had the picture to the picture. \"I he was a\n",
      "Ep 4 (Step 000035): Train loss 4.069, Val loss 6.226\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Ep 5 (Step 000040): Train loss 3.732, Val loss 6.160\n",
      "Every effort moves you know                        He laughed again, and he had the donkey and he had been his pictures of his pictures and down the room, and he\n",
      "Ep 6 (Step 000045): Train loss 2.850, Val loss 6.179\n",
      "Every effort moves you know it was not that the picture--I had the fact with the last--his, and I was--and here are the Riv, and I had been his head to have.             \n",
      "Ep 6 (Step 000050): Train loss 2.427, Val loss 6.141\n",
      "Every effort moves you know,\" was one of the picture.  I turned to the last word.        He laughed again, I had back his head to the donkey.  \"I didn't--the. \"I\n",
      "Ep 7 (Step 000055): Train loss 2.104, Val loss 6.134\n",
      "Every effort moves you know,\" was not that I felt as it was the fact with a little a. Gisburn's past!     \"I was back the head to the donkey.  \"I looked up--because he had always his\n",
      "Ep 7 (Step 000060): Train loss 1.882, Val loss 6.233\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little a flash that he _rose of the fact, the cigars you like.\"  He placed them at my elbow and as I had been the--because he had always _\n",
      "Ep 8 (Step 000065): Train loss 1.320, Val loss 6.238\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no great surprise to me to have to see a smile behind his pictures.  \"Oh, I saw that, and down, and I was.\n",
      "Ep 8 (Step 000070): Train loss 0.985, Val loss 6.242\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000075): Train loss 0.717, Val loss 6.293\n",
      "Every effort moves you know,\" was one of my hostess was \"interesting\": on that point I could have given Miss Croft the fact, had been to the display of his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, as I turned, my eye fell on a small picture\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "model.parameters(), # method returns all trainable weight params of the model\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "ead2d436-f2f2-4ed7-9438-16898350ad07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "    epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "9bd47bcc-a2db-4613-a88f-c1856379a324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e7ab5005-7fee-4071-b9e6-9dff68b109ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "model=model,\n",
    "idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "max_new_tokens=25,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4e3b6919-21ac-4ea0-a24d-db982c079fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "\"closer\": 0,\n",
    "\"every\": 1,\n",
    "\"effort\": 2,\n",
    "\"forward\": 3,\n",
    "\"inches\": 4,\n",
    "\"moves\": 5,\n",
    "\"pizza\": 6,\n",
    "\"toward\": 7,\n",
    "\"you\": 8,\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "3e012bb6-cc41-41c1-9ae4-4ce948d781f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "ffef7b84-d873-458b-a92e-a054b87e9f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([    0.0609,     0.0016,     0.0001,     0.5721,     0.0034,     0.0001,\n",
       "            0.0001,     0.3576,     0.0040])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])\n",
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "5fda51d0-7cd0-404f-8f13-fe1a4a2d0012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "41eae456-33b1-45df-9b5b-89654efb6120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
    "                for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "0aae763f-88ce-493e-af37-0956a1a2fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "9eb717fc-02ef-45d1-8dd6-70e2963490d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1, 0.1, 5] # orginal, lower and higher confidence\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "                for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i],\n",
    "                bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44986be4-c3e0-49bf-ba72-f1c21ac9d317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f7d95-3a07-4ab6-bb96-07cd3e3161ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "5f2618ba-f84b-463d-ba51-e95735ad17d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why top-k? using top-k , telling the model to select only from the top-k samples(highest probability).\n",
    "# rather than looking for the entire logits, to do this- select highest probabilities samples from the logits\n",
    "# and the rest will be masked (just like chapter-3 causal attention) \n",
    "# replaces with -inf before softmax after the rest of the logits would be assingned 0.00 nd the remaining probabilities sum up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "abbbedc9-a6c6-42b9-92e0-fdeee75c668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "258e61c8-841f-4e0e-a038-011c8261aa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1], # identifies logits less than the min in the top 3\n",
    "    input=torch.tensor(float('-inf')), # assigns -inf to lower logits\n",
    "    other=next_token_logits # retains the orginal logits for all othre tokens\n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c906d4ce-e13e-41ec-b9bc-6920a2024534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "076c3960-0f91-4233-a310-34cb11988bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "                temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "                logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "            \n",
    "        if top_k is not None: # filters logits with top-k sampling \n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0: # applies temperature scaling\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:     # Carries out greedy next-token selection as before when temperature scaling is disabled\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id: # stops generating early if end-of-seq token is encountered\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "16632dc1-63cb-4a0e-bc19-6ea03f3081dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you stand to work on surprise, a one of us had gone with random-\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "model=model,\n",
    "idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "max_new_tokens=15,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "top_k=25,\n",
    "temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "8ec46ea3-ef9a-45db-a17c-595a538b5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "91fc4c12-5a7d-4dcc-aa22-d29338af93cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "0f1a82b7-9cf5-4586-a54f-420e4ba11afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "fb2bde7b-a99a-48da-96b8-0a526ad07f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa88771-3626-405f-99b9-06050aa7dfce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
