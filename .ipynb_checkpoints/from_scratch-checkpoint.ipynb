{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60cdd03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x122e72ad0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "\"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d165cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf28145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc41c52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152ae936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f581d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n",
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "print(result)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e286bead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying it to the dataset\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba15217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'HAD',\n",
       " 'always',\n",
       " 'thought',\n",
       " 'Jack',\n",
       " 'Gisburn',\n",
       " 'rather',\n",
       " 'a',\n",
       " 'cheap',\n",
       " 'genius']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca15d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "933fffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2674fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text) # to numerical values \n",
    "        preprocessed = [\n",
    "        item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids): # back to natural language\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd219693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8c785a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "368ed6f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# error due to the text is not in training set. The word 'hello' not used in the short story 'The verdict'\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      6\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text) \u001b[38;5;66;03m# to numerical values \u001b[39;00m\n\u001b[32m      7\u001b[39m preprocessed = [\n\u001b[32m      8\u001b[39m item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m      9\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text)) # error due to the text is not in training set. The word 'hello' not used in the short story 'The verdict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1616306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding special context tokens :- that is we add <|unk|> to new or unknown words while <|endoftext|> for sentence completionb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ec28495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "# modifying the vocabulary \n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ef4f2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21849a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "        item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "        else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4bae2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90b424fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text)) # 1131 = <|unk|> and 1130 = <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13a9b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65adf89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66d5d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tik_tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99e09bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "\"of someunknownPlace.\"\n",
    ")\n",
    "integers = tik_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0835db9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "15496",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m string = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintegers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m string\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mSimpleTokenizerV2.decode\u001b[39m\u001b[34m(self, ids)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     text = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mint_to_str\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ids])\n\u001b[32m     16\u001b[39m     text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+([,.:;?!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m'\u001b[39m, \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[31mKeyError\u001b[39m: 15496"
     ]
    }
   ],
   "source": [
    "string = tokenizer.decode(integers)\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6afbf9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959, 13]\n"
     ]
    }
   ],
   "source": [
    "unk = \"Akwirw ier.\"\n",
    "print(tik_tokenizer.encode(unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac9c48a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "33901",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtik_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43munk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mSimpleTokenizerV2.decode\u001b[39m\u001b[34m(self, ids)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     text = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mint_to_str\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ids])\n\u001b[32m     16\u001b[39m     text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+([,.:;?!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m'\u001b[39m, \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[31mKeyError\u001b[39m: 33901"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tik_tokenizer.encode(unk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "095d7f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "# now tokenize the short story dataset using the BPE tokenizer\n",
    "enc_text = tik_tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f800643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5095"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "len(enc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fef225f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y: [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "263a9cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6abfba2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4920",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m context = enc_sample[:i]\n\u001b[32m      3\u001b[39m desired = enc_sample[i]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenizer.decode(context), \u001b[33m\"\u001b[39m\u001b[33m---->\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdesired\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mSimpleTokenizerV2.decode\u001b[39m\u001b[34m(self, ids)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     text = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mint_to_str\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ids])\n\u001b[32m     16\u001b[39m     text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+([,.:;?!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m'\u001b[39m, \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[31mKeyError\u001b[39m: 4920"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56e94067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tik_tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c76cc9-cf58-4848-aabc-a3f70a0a1bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4384763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "stride=128, shuffle=True, drop_last=True,\n",
    "num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            drop_last=drop_last,\n",
    "                            num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ea39b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c47f5680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "second_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb95ceda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4,\n",
    "    shuffle=False\n",
    "    )\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22b9c570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# token IDs into token embedding vectors\n",
    "\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bef3596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3]))) # embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40931bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b16f42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usefull embdng sizes and encode the input tokens into a\n",
    "# 256-dimensional vector representation, which is smaller than what the original GPT-3\n",
    "# model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable\n",
    "# for experimentation. furthermore, we assume that the token IDs were created by the\n",
    "# BPE tokenizer we implemented earlier, which has a vocabulary size of 50,257:\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "655189de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "raw_text, batch_size=8, max_length=max_length,\n",
    "stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ff4e355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f503675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd6fdb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d9c3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "076db777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # your     (x^1)\n",
    " [0.55, 0.87, 0.66], # journey  (x^2)\n",
    " [0.55, 0.87, 0.66], # starts   (x^3)\n",
    " [0.22, 0.58, 0.33], # with     (x^4)\n",
    " [0.77, 0.25, 0.10], # one      (x^5)\n",
    " [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6a7adb5-77ff-415f-9861-b679ca1a80a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8eb9d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] \n",
    "d_in = inputs.shape[1] # input embedding size , d = 3\n",
    "d_out = 2 # output embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e2b1f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6043e426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2961, 0.5166],\n",
       "        [0.2517, 0.6886],\n",
       "        [0.0740, 0.8665]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ca54d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "157fb950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c973cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3669, 0.7646],\n",
       "        [0.4433, 1.1419],\n",
       "        [0.4433, 1.1419],\n",
       "        [0.2408, 0.6706],\n",
       "        [0.1827, 0.3292],\n",
       "        [0.3275, 0.9642]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f4b96fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22) # unnormalized attention score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06191791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8524, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # computaton to all attention scores\n",
    "print(attn_scores_2) # 2nd element matches we computed prev (attn_scores_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e1e71ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1490, 0.2249, 0.2249, 0.1302, 0.0900, 0.1808])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1] \n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1) # qk^T / root_dk(dim of the key matrix)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "203e02dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3082, 0.8267])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e2697e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compact self-attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06eee1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax( attn_scores / keys.shape[-1]**0.5, dim=-1 )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b6b9754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3015, 0.8104],\n",
      "        [0.3082, 0.8267],\n",
      "        [0.3082, 0.8267],\n",
      "        [0.2965, 0.7986],\n",
      "        [0.2944, 0.7936],\n",
      "        [0.3009, 0.8091]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs)) # inputs contains 6 embedding vectors , results in a matrix storing 6 context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f250d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention using linear layer.\n",
    "# instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear\n",
    "# has an optimized weight initialization scheme, contributing to more stable and\n",
    "# effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1eadaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax( attn_scores / keys.shape[-1]**0.5, dim=-1 )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8bd18043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5340, -0.1049],\n",
      "        [-0.5326, -0.1078],\n",
      "        [-0.5326, -0.1078],\n",
      "        [-0.5300, -0.1074],\n",
      "        [-0.5313, -0.1064],\n",
      "        [-0.5302, -0.1079]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs)) # SelfAttention_v1 and SelfAttention_v2 give different outputs because\n",
    "                     # they use different initial weights for the weight matrices since nn.Linear uses a more\n",
    "                     # sophisticated weight initialization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5e5fd9de-a9c2-407b-9372-5b82f24d6398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1716, 0.1762, 0.1762, 0.1555, 0.1626, 0.1579],\n",
      "        [0.1635, 0.1749, 0.1749, 0.1611, 0.1604, 0.1651],\n",
      "        [0.1635, 0.1749, 0.1749, 0.1611, 0.1604, 0.1651],\n",
      "        [0.1636, 0.1703, 0.1703, 0.1651, 0.1632, 0.1674],\n",
      "        [0.1667, 0.1722, 0.1722, 0.1617, 0.1633, 0.1639],\n",
      "        [0.1624, 0.1708, 0.1708, 0.1654, 0.1624, 0.1681]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "                                # Reuses the query and key weight matrices\n",
    "                                # of the SelfAttention_v2 object from the\n",
    "                                # previous section for convenience\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7ed6723-9995-4b00-8bf6-1fdb6b93a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length)) #tril fn to create a mask where the values above the diagonal are zero\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a07c154e-7a48-44fc-b872-d57ad78c0494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1716, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1635, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1635, 0.1749, 0.1749, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1636, 0.1703, 0.1703, 0.1651, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1722, 0.1722, 0.1617, 0.1633, 0.0000],\n",
       "        [0.1624, 0.1708, 0.1708, 0.1654, 0.1624, 0.1681]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f89644d0-f015-4b18-b5a8-8ac58f24bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3186, 0.3407, 0.3407, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2444, 0.2544, 0.2544, 0.2467, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2060, 0.1934, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1708, 0.1708, 0.1654, 0.1624, 0.1681]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm) # renormalize the attention weights to sum up to 1 again in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "217f7b26-d4b4-46aa-824b-55675724fd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1716],\n",
       "        [0.3384],\n",
       "        [0.5133],\n",
       "        [0.6694],\n",
       "        [0.8361],\n",
       "        [1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d123de43-a8f5-444e-9847-281df10b8535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.2602, 0.2602,   -inf,   -inf,   -inf],\n",
      "        [0.0510, 0.1080, 0.1080, 0.0643,   -inf,   -inf],\n",
      "        [0.1415, 0.1875, 0.1875, 0.0987, 0.1121,   -inf],\n",
      "        [0.0476, 0.1192, 0.1192, 0.0731, 0.0477, 0.0966]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91e397e7-4fa7-40ca-8530-2d14edfd57fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3186, 0.3407, 0.3407, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2444, 0.2544, 0.2544, 0.2467, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2060, 0.1934, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1708, 0.1708, 0.1654, 0.1624, 0.1681]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43afff34-938f-4468-93f7-4efbee9c25ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1a2dc2a6-0cc5-4f13-89d1-5667e20193a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6373, 0.6814, 0.6814, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5089, 0.5089, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4119, 0.0000, 0.3869, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3417, 0.3417, 0.3307, 0.3248, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e4d7304f-762a-48a3-af6f-9b4af931589e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0466e467-dbd7-4d70-8f80-33025cd5dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer( # buffers are used automatically move to the apropriate device (cpu or gpu) along with the model\n",
    "        'mask',\n",
    "        torch.triu(torch.ones(context_length, context_length),\n",
    "        diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c6f334bc-5c8e-4028-9bdd-d1ef0c5a3c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5845e9e3-810b-4c0a-b532-ee12f357a6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6306, -0.0630],\n",
       "         [-0.5679, -0.0840],\n",
       "         [-0.5529, -0.0979],\n",
       "         [-0.5302, -0.1079]],\n",
       "\n",
       "        [[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6306, -0.0630],\n",
       "         [-0.5679, -0.0840],\n",
       "         [-0.5529, -0.0979],\n",
       "         [-0.5302, -0.1079]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "675d9900-5a95-4964-bf21-8540f21532f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "        dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(\n",
    "                d_in, d_out, context_length, dropout, qkv_bias\n",
    "            )\n",
    "            for _ in range(num_heads)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1) # processing sequentially not simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb17d73c-275e-47d8-8d50-9d2509ef6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "67f2ab62-9815-4e46-8e1d-e05788b88e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6306, -0.0630,  0.6206,  0.3875],\n",
      "         [-0.5679, -0.0840,  0.5479,  0.3597],\n",
      "         [-0.5529, -0.0979,  0.5322,  0.3434],\n",
      "         [-0.5302, -0.1079,  0.5077,  0.3499]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6306, -0.0630,  0.6206,  0.3875],\n",
      "         [-0.5679, -0.0840,  0.5479,  0.3597],\n",
      "         [-0.5529, -0.0979,  0.5322,  0.3434],\n",
      "         [-0.5302, -0.1079,  0.5077,  0.3499]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "da2dd240-ead2-4617-b3c5-f7ca33ec8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor is two because we have 2 input texts  (the input texts are duplicated, which is why the context vectors are exactly the same for those)\n",
    "# second dim refers to the 6 tokens in each input\n",
    "# third dim refers to the 4 dim embedding of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "914f4428-55e0-48d1-b7d8-59396aff2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "        context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # reduces the projection dim to match the desired output dim\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # uses a Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                diagonal=1)\n",
    "        )\n",
    "        \n",
    "    # tensor shape: (b, num_tokens, d_out)\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # we implicitly split the matrix by adding a num_heads dimension.\n",
    "        #then we unroll the last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim).\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(\n",
    "            b, num_tokens, self.num_heads, self.head_dim\n",
    "        )\n",
    "        #transposes from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2) \n",
    "        values = values.transpose(1, 2)\n",
    "    \n",
    "        attn_scores = queries @ keys.transpose(2, 3) # computes dot for each head\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # masks truncated to the no of tokens\n",
    "    \n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) # uses mask to fill attn_scores\n",
    "    \n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "    \n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # tensor shape: (b, num_tokens, n_heads, head_dim)\n",
    "    \n",
    "        # Combines heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "        b, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec) # adds an optional linear projection\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "23dd97cd-c7d2-43f4-a3d1-cc059b04fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "[0.8993, 0.0390, 0.9268, 0.7388],\n",
    "[0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "[[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "[0.4066, 0.2318, 0.4545, 0.9737],\n",
    "[0.4606, 0.5159, 0.4220, 0.5786]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e5d94-fb83-47b8-a279-4489b9b859a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a48c4a-2dc3-4af5-aa53-e718688d439d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f7674f3a-06a9-455d-9247-6c0d74490949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "print(a @ a.transpose(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "68f97b6e-b2d4-4c4b-9e7c-c9adbc1e963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aaaf6fde-5c54-4218-bd67-b9af137c8422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "47b51e7d-0abd-49cf-b23a-e1c579fb173e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2857, 0.3590],\n",
      "         [0.2694, 0.3872],\n",
      "         [0.2640, 0.3926],\n",
      "         [0.2576, 0.4027]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2857, 0.3590],\n",
      "         [0.2694, 0.3872],\n",
      "         [0.2640, 0.3926],\n",
      "         [0.2576, 0.4027]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6c44676e-1bb0-47c7-9d96-161d68399a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "        context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # reduces the projection dim to match the desired output dim\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # uses a Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                diagonal=1)\n",
    "        )\n",
    "        \n",
    "    # tensor shape: (b, num_tokens, d_out)\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # we implicitly split the matrix by adding a num_heads dimension.\n",
    "        #then we unroll the last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim).\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(\n",
    "            b, num_tokens, self.num_heads, self.head_dim\n",
    "        )\n",
    "        #transposes from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2) \n",
    "        values = values.transpose(1, 2)\n",
    "    \n",
    "        attn_scores = queries @ keys.transpose(2, 3) # computes dot for each head\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # masks truncated to the no of tokens\n",
    "    \n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) # uses mask to fill attn_scores\n",
    "    \n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "    \n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # tensor shape: (b, num_tokens, n_heads, head_dim)\n",
    "    \n",
    "        # Combines heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "        b, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec) # adds an optional linear projection\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "79b78c79-a816-436c-aac0-acba6dcbe9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0208, -0.1094, -0.1502,  ...,  0.3617,  0.2821,  0.0099],\n",
      "         [-0.0732, -0.1550, -0.1058,  ...,  0.4179,  0.2185,  0.0626],\n",
      "         [-0.1040, -0.1697, -0.0913,  ...,  0.4334,  0.1953,  0.0796],\n",
      "         [-0.1055, -0.1599, -0.0703,  ...,  0.3901,  0.1607,  0.0774],\n",
      "         [-0.0781, -0.1212, -0.0909,  ...,  0.3380,  0.1471,  0.0598],\n",
      "         [-0.0927, -0.1375, -0.0687,  ...,  0.3536,  0.1342,  0.0648]],\n",
      "\n",
      "        [[ 0.0208, -0.1094, -0.1502,  ...,  0.3617,  0.2821,  0.0099],\n",
      "         [-0.0732, -0.1550, -0.1058,  ...,  0.4179,  0.2185,  0.0626],\n",
      "         [-0.1040, -0.1697, -0.0913,  ...,  0.4334,  0.1953,  0.0796],\n",
      "         [-0.1055, -0.1599, -0.0703,  ...,  0.3901,  0.1607,  0.0774],\n",
      "         [-0.0781, -0.1212, -0.0909,  ...,  0.3380,  0.1471,  0.0598],\n",
      "         [-0.0927, -0.1375, -0.0687,  ...,  0.3536,  0.1342,  0.0648]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 768\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=12)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8e9d53e8-2861-47eb-bc5e-042e07d45c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257,     # Vocabulary size\n",
    "\"context_length\": 1024,  # Context length\n",
    "\"emb_dim\": 768,          # Embedding dimension\n",
    "\"n_heads\": 12,           # Number of attention heads\n",
    "\"n_layers\": 12,          # Number of layers\n",
    "\"drop_rate\": 0.1,        # Dropout rate\n",
    "\"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3e7ba913-663b-4c75-9360-2adea3b5eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg)\n",
    "            for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "        torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c4faf8a-c314-4513-8b83-a40fad379fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a39d5c85-5114-44f4-bfe2-e7721089b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e7759b6-a2df-49da-acbc-656dbec2d582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
       "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
       "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
       "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
       "\n",
       "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
       "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
       "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
       "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch) # logits are unormalized model's pred before an actvn fn is applied , these values can range from -ve to +ve infinity\n",
    "print(\"Output shape:\", logits.shape) # and represent the model's confidence in assigning an input to a particular class.\n",
    "logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b0984587-c3a0-4265-974d-a1542896e6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
       "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c39e5ef9-4493-4f33-b993-9bde7e6199eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
       "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a446f493-924a-4562-8ccb-7a325e14539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e377b37f-8ccf-4f12-9823-1c98855a3b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6e78413d-5d53-4502-8a1b-b975de32c174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False) \n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "29cbe8ec-7400-45b3-8f1f-39f80262737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9f45353e-31dd-414b-82be-2ee2683c30b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "44d8c09a-fe44-40b5-8e3e-c5640f4ec872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7927c57e-edaa-4e4c-9133-3e7b0f85ce9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'tight_lay'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m     plt.ylabel(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(x)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m     plt.grid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtight_lay\u001b[49m\n\u001b[32m     16\u001b[39m plt.show\n",
      "\u001b[31mAttributeError\u001b[39m: module 'matplotlib.pyplot' has no attribute 'tight_lay'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_lay\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bf15cc53-e431-443c-b95b-41d4637c23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The smoothness of GELU can lead to better optimization properties during training,\n",
    "# as it allows for more nuanced adjustments to the model’s parameters. In contrast,\n",
    "# ReLU has a sharp corner at zero , which can sometimes make opti-mization harder, \n",
    "# especially in networks that are very deep or have complex architectures.\n",
    "# Moreover, unlike ReLU, which outputs zero for any negative input, GELU\n",
    "# allows for a small, non-zero output for negative values. This characteristic means that\n",
    "# during the training process, neurons that receive negative input can still contribute to\n",
    "# the learning process, albeit to a lesser extent than positive inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e0f4c6f5-92f4-4a65-9463-6f37b71d76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c7b2fc53-be3a-4f01-9fd1-7a5fd201299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b1b76820-7c62-4680-80a8-ab66002a5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut Connections or Skip Conncetions or Residual Connections\n",
    "# as the layer progress there's a high chance of problem like vanishing gradient.\n",
    "# vanshing gradient :- vanishing gradient problem refers to the issue where gradients\n",
    "#     (which guide weight updates during training) become progressively smaller as they\n",
    "#     propagate backward through the layers, making it difficult to effectively train earlier\n",
    "#     layers.\n",
    "# to prevent vanishing gradiant prblem  the soln is  skip or residual conncetions :-\n",
    "#         creates alternative or shotcut path for grdient flow  thruogh the network by skipping one or more layers,\n",
    "#         which is achieved by adding output of one layer to the output of a later layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3c3dbe90-0a74-4077-82b0-d49381f11107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "        nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]),\n",
    "        GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]),\n",
    "        GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]),\n",
    "        GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]),\n",
    "        GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]),\n",
    "        GELU())\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x) # compute the output of the current layer\n",
    "            if self.use_shortcut and x.shape == layer_output.shape: # check if shortcuts can be applied\n",
    "                x = x + layer_output \n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "079a9ae0-d5e5-4d87-977d-3f36db3e1199",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specifies random seeds for initial weights for reproducibility\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d40838b7-ddfc-4a2a-a838-d029f84c3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x) # fwd pass\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target) # calculates loss based on how close the target and output are\n",
    "    loss.backward() # backward pass to calculate the gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323de465-fe04-4e88-9856-fd0a8e9251e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2370d-b539-4e2a-8f88-18cfda690433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2d7b6d71-3d98-4929-bab5-af6ceafacfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
      "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1b36521f-694d-4f53-9630-670d5b1d385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the gradient becomes smaller when we progress from the last layer (vanishing gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "75b160e8-82cd-492c-b8d2-0668467ecc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True # applying skip connections\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fc63b10e-2815-41fb-991f-366d3c0b8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        shortcut = x # shortcut conn for attention block\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # add the orginal input back\n",
    "\n",
    "        shortcut = x # shortcut conn for ff block\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # adds the orginal back\n",
    "        return x\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9bc3d7ca-9958-4f27-958a-9098c995a35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) # smaple i/p shape [batch_size, num_tokens, num_emb]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b77e0173-e03b-4726-b267-f7900e4fb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb( \n",
    "        torch.arange(seq_len, device=in_idx.device) # device setting will allow us to train the model on a CPU or GPU, depending on which device the input data sits on.\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9b97256e-7839-4fa6-85f2-38aaf920fd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4223, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c7fe94ce-5818-4514-8726-ccd3cfc12ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters()) # analyzing size using numel() - number of elements, we can collect\n",
    "print(f\"Total number of parameters: {total_params:,}\") # total number of parameters in the model's parameter tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "693db8f9-7654-4865-b260-047c036b5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why 163m instead of 124m ? - reason is a concept called \"weight tying\" which was used in the original GPT-2 architecture.\n",
    "# It means that the original GPT-2 architecture reuses the weights from the tokn embdng layer in its output layer.\n",
    "# To understand better, let’s take a look at the shapes of the token emdng layer and linear ouput layer that we initialzed\n",
    "# on the model via the GPTModel earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dbe101d4-9bd7-4949-acc3-04905dbd182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8fdf2d24-06b4-4436-9f21-17f48a852366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token embedding and output layers are very large due to the number of rows for\n",
    "# the 50,257 in the tokenizer’s vocabulary. Let’s remove the output layer parameter\n",
    "# count from the total GPT-2 model count according to the weight tying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2490e3f1-d129-455f-a747-f310f1d7b46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "total_params - sum(p.numel()\n",
    "for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {total_params_gpt2:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e7d5ff-9b69-4cad-8a0d-567bb4b6fd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
